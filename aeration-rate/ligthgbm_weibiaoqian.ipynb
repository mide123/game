{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/train_dataset.csv\")\n",
    "\n",
    "data['is_train'] = True\n",
    "evaluation = pd.read_csv(\"./data/evaluation_public.csv\")\n",
    "evaluation['is_train'] = False\n",
    "sample = pd.read_csv(\"./data/sample_submission.csv\")\n",
    "\n",
    "data = pd.concat([data, evaluation]).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 特征处理"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "data['date'] = pd.to_datetime(data['time'])\n",
    "data['hour'] = data['date'].dt.hour\n",
    "data['year'] = data['date'].dt.year\n",
    "data['month'] = data['date'].dt.month\n",
    "data['minute'] = data['date'].dt.minute\n",
    "data['weekday'] = data['date'].dt.weekday\n",
    "data['day'] = data['date'].dt.day\n",
    "data['hour'] = data['date'].dt.hour\n",
    "data['ts'] = data['hour']*3600 + data['minute']*60 + data['date'].dt.second"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "import gc\n",
    "features = [ 'JS_NH3', 'CS_NH3', 'JS_TN', 'CS_TN', 'JS_LL', 'CS_LL', 'MCCS_NH4', 'MCCS_NO3', 'JS_COD', 'CS_COD', 'JS_SW', 'CS_SW', 'B_HYC_NH4', 'B_HYC_XD', 'B_HYC_MLSS', 'B_HYC_JS_DO', 'B_HYC_DO', 'B_CS_MQ_SSLL', 'B_QY_ORP', 'N_HYC_NH4', 'N_HYC_XD', 'N_HYC_MLSS', 'N_HYC_JS_DO', 'N_HYC_DO', 'N_CS_MQ_SSLL', 'N_QY_ORP','weekday','hour', 'ts']\n",
    "features = [f for f in features if f not in ['time', 'Label1', 'Label2','CS_LL','CS_NH3', 'JS_SW']] #\n",
    "\n",
    "for f in features:\n",
    "    data[f] = data[f].fillna(method='ffill')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 提取特征的diff值"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_26540/1078796225.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_min_name}_cha'] = data[f] - data[f_min_name]\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_26540/1078796225.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_mean_name] = train_rolling.mean().fillna(0).values\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_26540/1078796225.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_max_name] = train_rolling.max().fillna(0).values\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_26540/1078796225.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_min_name] = train_rolling.min().fillna(0).values\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_26540/1078796225.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_std_name] = train_rolling.std().fillna(0).values\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_26540/1078796225.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_corr_name] = train_rolling.corr().fillna(0).values\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_26540/1078796225.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_skew_name] = train_rolling.skew().fillna(0).values\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_26540/1078796225.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_mean_name}_cha'] = data[f] - data[f_mean_name]\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_26540/1078796225.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_max_name}_cha'] = data[f] - data[f_max_name]\n"
     ]
    }
   ],
   "source": [
    "df_list = []\n",
    "i = 0\n",
    "add_featuers = []\n",
    "length = 0\n",
    "for f in ['JS_NH3', 'CS_TN', 'JS_LL', 'JS_COD', 'CS_COD', 'B_HYC_NH4', 'B_HYC_XD', 'B_HYC_MLSS', 'B_HYC_JS_DO', 'B_HYC_DO', 'B_CS_MQ_SSLL',\n",
    "          'N_HYC_NH4', 'N_HYC_XD', 'N_HYC_MLSS', 'N_HYC_DO', 'N_CS_MQ_SSLL', 'N_QY_ORP']:\n",
    "    for r in [15]:\n",
    "        train_rolling = data[f].rolling(window=r, center=False)\n",
    "        f_mean_name = 'rolling{}_{}_mean'.format(r,f)\n",
    "        f_max_name = 'rolling{}_{}_max'.format(r,f)\n",
    "        f_min_name = 'rolling{}_{}_min'.format(r,f)\n",
    "        f_std_name = 'rolling{}_{}_std'.format(r,f)\n",
    "        f_corr_name = 'rolling{}_{}_corr'.format(r,f)\n",
    "        f_cov_name = 'rolling{}_{}_cov'.format(r,f)\n",
    "        f_skew_name = 'rolling{}_{}_skew'.format(r,f)\n",
    "        f_kurt_name = 'rolling{}_{}_kurt'.format(r,f)\n",
    "        data[f_mean_name] = train_rolling.mean().fillna(0).values\n",
    "        data[f_max_name] = train_rolling.max().fillna(0).values\n",
    "        data[f_min_name] = train_rolling.min().fillna(0).values\n",
    "        data[f_std_name] = train_rolling.std().fillna(0).values\n",
    "        data[f_corr_name] = train_rolling.corr().fillna(0).values\n",
    "        data[f_skew_name] = train_rolling.skew().fillna(0).values\n",
    "\n",
    "        data[f'{f}_{f_mean_name}_cha'] = data[f] - data[f_mean_name]\n",
    "        data[f'{f}_{f_max_name}_cha'] = data[f] - data[f_max_name]\n",
    "        data[f'{f}_{f_min_name}_cha'] = data[f] - data[f_min_name]\n",
    "\n",
    "        if i == 0:\n",
    "            add_featuers.append(f_mean_name)\n",
    "            add_featuers.append(f_max_name)\n",
    "            add_featuers.append(f_min_name)\n",
    "            add_featuers.append(f_std_name)\n",
    "            add_featuers.append(f_corr_name)\n",
    "            add_featuers.append(f_skew_name)\n",
    "            add_featuers.append(f'{f}_{f_mean_name}_cha')\n",
    "            add_featuers.append(f'{f}_{f_max_name}_cha')\n",
    "            add_featuers.append(f'{f}_{f_min_name}_cha')\n",
    "features.extend(add_featuers)\n",
    "\n",
    "# 对所有的特征进行划分\n",
    "for f in features:\n",
    "    if f not in ['weekday','hour', 'ts']:\n",
    "        q = len(data[f].drop_duplicates())\n",
    "        data[f] = pd.qcut(data[f], q=int(q/10), labels=False, duplicates=\"drop\")\n",
    "\n",
    "all_train = data[data['is_train']].reset_index(drop=True)\n",
    "test = data[~data['is_train']].reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 删除分布不均衡的特征"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "pseudo = all_train[all_train['Label1'].isna()].copy().reset_index(drop=True)\n",
    "all_train = all_train.dropna(subset=['Label1', 'Label2']).reset_index(drop=True)\n",
    "def transform(x: pd.Series, c=20):\n",
    "    return np.log1p(x/c)\n",
    "\n",
    "def inverse_transform(x: pd.Series, c = 20):\n",
    "    return np.expm1(x)*c\n",
    "\n",
    "all_train = all_train.dropna(subset=['Label1', 'Label2']).reset_index(drop=True)\n",
    "all_train['Label1'] = transform(all_train['Label1'])\n",
    "label_c = 8\n",
    "all_train['Label2'] = transform(all_train['Label2'], c=label_c)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "# inverse_transform(pd.Series([6,6.1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 伪标签进行预测"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1e-08, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1e-08\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\ttraining's rmse: 0.133487\ttraining's l2: 0.0178188\tvalid_1's rmse: 0.11992\tvalid_1's l2: 0.0143807\n",
      "[400]\ttraining's rmse: 0.108866\ttraining's l2: 0.0118519\tvalid_1's rmse: 0.116823\tvalid_1's l2: 0.0136476\n",
      "Early stopping, best iteration is:\n",
      "[362]\ttraining's rmse: 0.111394\ttraining's l2: 0.0124087\tvalid_1's rmse: 0.116735\tvalid_1's l2: 0.0136271\n",
      "score_list = [1532.3947407984178]\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1e-08, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1e-08\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\ttraining's rmse: 0.107732\ttraining's l2: 0.0116063\tvalid_1's rmse: 0.131644\tvalid_1's l2: 0.0173301\n",
      "Early stopping, best iteration is:\n",
      "[206]\ttraining's rmse: 0.107197\ttraining's l2: 0.0114912\tvalid_1's rmse: 0.131471\tvalid_1's l2: 0.0172846\n",
      "score_list = [1532.3947407984178, 1670.9157593161995]\n",
      "0.6239645113721378\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "labels = ['Label1', 'Label2']\n",
    "score_list = []\n",
    "test_size = 10000\n",
    "\n",
    "for label in labels:\n",
    "    X_train = all_train[features][:-test_size]\n",
    "    y_train = all_train[label][:-test_size]\n",
    "\n",
    "    X_test = all_train[features][-test_size:]\n",
    "    y_test = all_train[label][-test_size:]\n",
    "    if label == \"Label1\":\n",
    "        model = lgb.LGBMRegressor(\n",
    "            boosting=\"gbdt\",\n",
    "            max_depth=4,\n",
    "            learning_rate=0.01,\n",
    "            n_estimators=10000,\n",
    "            min_data_in_leaf=5,\n",
    "            subsample = 0.8,\n",
    "            feature_fraction=0.3,\n",
    "            bagging_seed=1,\n",
    "            reg_alpha=1,\n",
    "            reg_lambda=1,  # 此处不改了\n",
    "            min_sum_hessian_in_leaf=1e-8,\n",
    "            random_state=1212\n",
    "        )\n",
    "        model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], eval_metric=['rmse'],\n",
    "              early_stopping_rounds=150, verbose=200)\n",
    "        test_pred = model.predict(X_test)\n",
    "\n",
    "        score_list.append(np.sqrt(mean_squared_error(inverse_transform(y_test), inverse_transform(test_pred))))\n",
    "        print(f\"score_list = {score_list}\")\n",
    "        pseudo[label] = model.predict(pseudo[features])\n",
    "    else:\n",
    "        model = lgb.LGBMRegressor(\n",
    "            boosting=\"gbdt\",\n",
    "            max_depth=4,\n",
    "            learning_rate=0.02,\n",
    "            n_estimators=10000,\n",
    "            min_data_in_leaf=40,\n",
    "            subsample = 0.4,\n",
    "            feature_fraction=0.3,\n",
    "            bagging_seed=11,\n",
    "            reg_alpha=.1,\n",
    "            reg_lambda=.1,  # 此处不改了\n",
    "            min_sum_hessian_in_leaf=1e-8,\n",
    "            random_state=1589\n",
    "        )\n",
    "\n",
    "        model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], eval_metric=['rmse'],\n",
    "              early_stopping_rounds=150, verbose=200)\n",
    "        test_pred = model.predict(X_test)\n",
    "        score_list.append(np.sqrt(mean_squared_error(inverse_transform(y_test, c=label_c), inverse_transform(test_pred, c=label_c))))\n",
    "        print(f\"score_list = {score_list}\")\n",
    "        pseudo[label] = model.predict(pseudo[features])\n",
    "loss = np.mean(score_list)\n",
    "score = 1000/(1+loss)\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "# max(train_pseudo['Label2'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6000426182480831\n"
     ]
    }
   ],
   "source": [
    "loss = np.mean(score_list)\n",
    "score = 1000/(1+loss)\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 对加入伪标签的数据进行训练"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "# train_pseudo = pd.concat([all_train[:test_size], pseudo])\n",
    "# train_pseudo = train_pseudo.sample(len(train_pseudo)).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.01, min_child_weight=1 will be ignored. Current value: min_sum_hessian_in_leaf=0.01\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\ttraining's rmse: 0.0528962\ttraining's l2: 0.002798\tvalid_1's rmse: 0.160047\tvalid_1's l2: 0.0256152\n",
      "Early stopping, best iteration is:\n",
      "[83]\ttraining's rmse: 0.0860057\ttraining's l2: 0.00739699\tvalid_1's rmse: 0.15818\tvalid_1's l2: 0.0250208\n",
      "score_list = [2008.1427877617364]\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.1\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[200]\ttraining's rmse: 0.0456254\ttraining's l2: 0.00208168\tvalid_1's rmse: 0.117215\tvalid_1's l2: 0.0137392\n",
      "[400]\ttraining's rmse: 0.0400512\ttraining's l2: 0.0016041\tvalid_1's rmse: 0.116002\tvalid_1's l2: 0.0134564\n",
      "[600]\ttraining's rmse: 0.0371497\ttraining's l2: 0.0013801\tvalid_1's rmse: 0.115441\tvalid_1's l2: 0.0133267\n",
      "[800]\ttraining's rmse: 0.035104\ttraining's l2: 0.00123229\tvalid_1's rmse: 0.115165\tvalid_1's l2: 0.0132631\n",
      "[1000]\ttraining's rmse: 0.0333195\ttraining's l2: 0.00111019\tvalid_1's rmse: 0.114889\tvalid_1's l2: 0.0131995\n",
      "[1200]\ttraining's rmse: 0.0317922\ttraining's l2: 0.00101074\tvalid_1's rmse: 0.114654\tvalid_1's l2: 0.0131455\n",
      "[1400]\ttraining's rmse: 0.0304926\ttraining's l2: 0.000929801\tvalid_1's rmse: 0.114628\tvalid_1's l2: 0.0131396\n",
      "[1600]\ttraining's rmse: 0.0293166\ttraining's l2: 0.000859462\tvalid_1's rmse: 0.11457\tvalid_1's l2: 0.0131264\n",
      "[1800]\ttraining's rmse: 0.0282716\ttraining's l2: 0.000799283\tvalid_1's rmse: 0.114442\tvalid_1's l2: 0.013097\n",
      "[2000]\ttraining's rmse: 0.0272905\ttraining's l2: 0.00074477\tvalid_1's rmse: 0.114405\tvalid_1's l2: 0.0130885\n",
      "[2200]\ttraining's rmse: 0.026375\ttraining's l2: 0.00069564\tvalid_1's rmse: 0.114378\tvalid_1's l2: 0.0130823\n",
      "[2400]\ttraining's rmse: 0.0255453\ttraining's l2: 0.000652563\tvalid_1's rmse: 0.114339\tvalid_1's l2: 0.0130733\n",
      "[2600]\ttraining's rmse: 0.0248133\ttraining's l2: 0.000615701\tvalid_1's rmse: 0.114326\tvalid_1's l2: 0.0130705\n",
      "[2800]\ttraining's rmse: 0.0240788\ttraining's l2: 0.000579788\tvalid_1's rmse: 0.114329\tvalid_1's l2: 0.0130711\n",
      "[3000]\ttraining's rmse: 0.0234295\ttraining's l2: 0.000548942\tvalid_1's rmse: 0.114302\tvalid_1's l2: 0.0130649\n",
      "[3200]\ttraining's rmse: 0.0228379\ttraining's l2: 0.000521569\tvalid_1's rmse: 0.114308\tvalid_1's l2: 0.0130663\n",
      "[3400]\ttraining's rmse: 0.0222695\ttraining's l2: 0.000495932\tvalid_1's rmse: 0.114264\tvalid_1's l2: 0.0130563\n",
      "[3600]\ttraining's rmse: 0.0216974\ttraining's l2: 0.000470779\tvalid_1's rmse: 0.114241\tvalid_1's l2: 0.013051\n",
      "[3800]\ttraining's rmse: 0.0212212\ttraining's l2: 0.00045034\tvalid_1's rmse: 0.114244\tvalid_1's l2: 0.0130518\n",
      "Early stopping, best iteration is:\n",
      "[3598]\ttraining's rmse: 0.0217053\ttraining's l2: 0.000471119\tvalid_1's rmse: 0.114241\tvalid_1's l2: 0.013051\n",
      "score_list = [2008.1427877617364, 1542.025131773378]\n",
      "0.5630364457155921\n"
     ]
    }
   ],
   "source": [
    "# import lightgbm as lgb\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# labels = ['Label1', 'Label2']\n",
    "# score_list = []\n",
    "# for label in reversed(labels):\n",
    "#     X_train = train_pseudo[features]\n",
    "#     y_train = train_pseudo[label]\n",
    "#\n",
    "#     X_test = all_train[features][-test_size:]\n",
    "#     y_test = all_train[label][-test_size:]\n",
    "#     if label == \"Label1\":\n",
    "#         model = lgb.LGBMRegressor(\n",
    "#             boosting=\"gbdt\",\n",
    "#             max_depth=6,\n",
    "#             learning_rate=0.03,\n",
    "#             n_estimators=10000,\n",
    "#             min_data_in_leaf=50,\n",
    "#             subsample = 0.8,\n",
    "#             feature_fraction=0.3,\n",
    "#             bagging_seed=1,\n",
    "#             reg_alpha=1,\n",
    "#             reg_lambda=1,  # 此处不改了\n",
    "#             min_sum_hessian_in_leaf=0.1,\n",
    "#             random_state=1212\n",
    "#         )\n",
    "#         model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], eval_metric=['rmse'],\n",
    "#               early_stopping_rounds=150, verbose=200)\n",
    "#         test_pred = model.predict(X_test)\n",
    "#\n",
    "#         score_list.append(np.sqrt(mean_squared_error(inverse_transform(y_test), inverse_transform(test_pred))))\n",
    "#         print(f\"score_list = {score_list}\")\n",
    "#\n",
    "#     else:\n",
    "#         model = lgb.LGBMRegressor(\n",
    "#             boosting=\"gbdt\",\n",
    "#             max_depth=4,\n",
    "#             learning_rate=0.02,\n",
    "#             n_estimators=10000,\n",
    "#             min_child_weight=1,\n",
    "#             min_data_in_leaf=40,\n",
    "#             subsample = 0.4,\n",
    "#             feature_fraction=0.3,\n",
    "#             bagging_seed=11,\n",
    "#             reg_alpha=1,\n",
    "#             reg_lambda=1,  # 此处不改了\n",
    "#             min_sum_hessian_in_leaf=0.01,\n",
    "#             random_state=222\n",
    "#         )\n",
    "#\n",
    "#         model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], eval_metric=['rmse'],\n",
    "#               early_stopping_rounds=150, verbose=200)\n",
    "#         test_pred = model.predict(X_test)\n",
    "#         score_list.append(np.sqrt(mean_squared_error(inverse_transform(y_test, c=label_c), inverse_transform(test_pred, c=label_c))))\n",
    "#         print(f\"score_list = {score_list}\")\n",
    "#\n",
    "# loss = np.mean(score_list)\n",
    "# score = 1000/(1+loss)\n",
    "# print(score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}