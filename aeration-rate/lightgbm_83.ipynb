{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/train_dataset.csv\")\n",
    "\n",
    "data['is_test'] = False\n",
    "evaluation = pd.read_csv(\"./data/evaluation_public.csv\")\n",
    "evaluation['is_test'] = True\n",
    "sample = pd.read_csv(\"./data/sample_submission.csv\")\n",
    "\n",
    "all_data = pd.concat([data, evaluation]).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 特征处理"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "all_data['date'] = pd.to_datetime(all_data['time'])\n",
    "all_data['hour'] = all_data['date'].dt.hour\n",
    "all_data['year'] = all_data['date'].dt.year\n",
    "all_data['month'] = all_data['date'].dt.month\n",
    "all_data['minute'] = all_data['date'].dt.minute\n",
    "all_data['weekday'] = all_data['date'].dt.weekday\n",
    "all_data['day'] = all_data['date'].dt.day\n",
    "all_data['hour'] = all_data['date'].dt.hour\n",
    "all_data['ts'] = all_data['hour']*3600 + all_data['minute']*60 + all_data['date'].dt.second"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import gc\n",
    "features = [ 'JS_NH3', 'CS_NH3', 'JS_TN', 'CS_TN', 'JS_LL', 'CS_LL', 'MCCS_NH4', 'MCCS_NO3', 'JS_COD', 'CS_COD', 'JS_SW', 'CS_SW', 'B_HYC_NH4', 'B_HYC_XD', 'B_HYC_MLSS', 'B_HYC_JS_DO', 'B_HYC_DO', 'B_CS_MQ_SSLL', 'B_QY_ORP', 'N_HYC_NH4', 'N_HYC_XD', 'N_HYC_MLSS', 'N_HYC_JS_DO', 'N_HYC_DO', 'N_CS_MQ_SSLL', 'N_QY_ORP','weekday','hour', 'ts']\n",
    "features = [f for f in features if f not in ['time', 'Label1', 'Label2','CS_LL','CS_NH3', 'JS_SW']] #\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "labels = ['Label1', 'Label2']\n",
    "train = all_data[~all_data['is_test']].copy(deep=True)\n",
    "test = all_data[all_data['is_test']].copy(deep=True)\n",
    "test['is_train'] = False\n",
    "train['is_train'] = True\n",
    "data = pd.concat([train, test]).reset_index(drop=True)\n",
    "del all_data\n",
    "gc.collect()\n",
    "\n",
    "for f in features:\n",
    "    data[f] = data[f].fillna(method='ffill')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 提取特征的diff值"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# tmp_features = list(features)\n",
    "# add_dict = {}\n",
    "# for f in tmp_features:\n",
    "#     data[f'{f}_diff']  = data[f].shift()\n",
    "#     features.append(f'{f}_diff')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# fff = ['MCCS_NO3', 'B_HYC_NH4','N_HYC_NH4']\n",
    "# for f in features:\n",
    "#     data[f] = data[f].fillna(method='ffill')\n",
    "# for i in range(len(fff)-1):\n",
    "#     for j in range(i+1, len(fff)):\n",
    "#         f1 = features[i]\n",
    "#         f2 = features[j]\n",
    "#         # feature_jian = f\"{f1}_{f2}_jian\"\n",
    "#         feature_radio = f\"{f1}_{f2}_radio\"\n",
    "#         data[feature_radio] = data[f1]/data[f2]\n",
    "#         # data[feature_jian] = data[f1]-data[f2]\n",
    "#         features.append(feature_radio)\n",
    "#         # features.append(feature_jian)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_2456/2090275524.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_min_name}_cha'] = data[f] - data[f_min_name]\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_2456/2090275524.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_mean_name] = train_rolling.mean().fillna(0).values\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_2456/2090275524.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_max_name] = train_rolling.max().fillna(0).values\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_2456/2090275524.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_min_name] = train_rolling.min().fillna(0).values\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_2456/2090275524.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_corr_name] = train_rolling.corr().fillna(0).values\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_2456/2090275524.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_std_name] = train_rolling.std().fillna(0).values\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_2456/2090275524.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_mean_name}_cha'] = data[f] - data[f_mean_name]\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_2456/2090275524.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_max_name}_cha'] = data[f] - data[f_max_name]\n"
     ]
    }
   ],
   "source": [
    "df_list = []\n",
    "i = 0\n",
    "add_featuers = []\n",
    "length = 0\n",
    "for f in ['JS_NH3', 'CS_TN', 'JS_LL', 'JS_COD', 'CS_COD', 'B_HYC_NH4', 'B_HYC_XD', 'B_HYC_MLSS', 'B_HYC_JS_DO', 'B_HYC_DO', 'B_CS_MQ_SSLL',\n",
    "          'N_HYC_NH4', 'N_HYC_XD', 'N_HYC_MLSS', 'N_HYC_DO', 'N_CS_MQ_SSLL', 'N_QY_ORP']:\n",
    "    for r in [15]:\n",
    "        train_rolling = data[f].rolling(window=r, center=False)\n",
    "        f_mean_name = 'rolling{}_{}_mean'.format(r,f)\n",
    "        f_max_name = 'rolling{}_{}_max'.format(r,f)\n",
    "        f_min_name = 'rolling{}_{}_min'.format(r,f)\n",
    "        f_std_name = 'rolling{}_{}_std'.format(r,f)\n",
    "        f_corr_name = 'rolling{}_{}_corr'.format(r,f)\n",
    "        data[f_mean_name] = train_rolling.mean().fillna(0).values\n",
    "        data[f_max_name] = train_rolling.max().fillna(0).values\n",
    "        data[f_min_name] = train_rolling.min().fillna(0).values\n",
    "        data[f_corr_name] = train_rolling.corr().fillna(0).values\n",
    "        data[f_std_name] = train_rolling.std().fillna(0).values\n",
    "        data[f'{f}_{f_mean_name}_cha'] = data[f] - data[f_mean_name]\n",
    "        data[f'{f}_{f_max_name}_cha'] = data[f] - data[f_max_name]\n",
    "        data[f'{f}_{f_min_name}_cha'] = data[f] - data[f_min_name]\n",
    "\n",
    "        if i == 0:\n",
    "            add_featuers.append(f_mean_name)\n",
    "            add_featuers.append(f_max_name)\n",
    "            add_featuers.append(f_min_name)\n",
    "            add_featuers.append(f_std_name)\n",
    "            add_featuers.append(f_corr_name)\n",
    "            add_featuers.append(f'{f}_{f_mean_name}_cha')\n",
    "            add_featuers.append(f'{f}_{f_max_name}_cha')\n",
    "            add_featuers.append(f'{f}_{f_min_name}_cha')\n",
    "\n",
    "    # for r in [5]:\n",
    "    #     train_rolling = data[f].shift(15).rolling(window=r, center=False)\n",
    "    #     f_mean_name = 'shift_rolling{}_{}_mean'.format(r,f)\n",
    "    #     f_max_name = 'shift_rolling{}_{}_max'.format(r,f)\n",
    "    #     f_min_name = 'shift_rolling{}_{}_min'.format(r,f)\n",
    "    #     f_std_name = 'shift_rolling{}_{}_std'.format(r,f)\n",
    "    #     data[f_mean_name] = train_rolling.mean().fillna(0).values\n",
    "    #     data[f_max_name] = train_rolling.max().fillna(0).values\n",
    "    #     data[f_min_name] = train_rolling.min().fillna(0).values\n",
    "    #     data[f_std_name] = train_rolling.std().fillna(0).values\n",
    "    #     data[f'shift_{f}_{f_mean_name}_cha'] = data[f] - data[f_mean_name]\n",
    "    #     data[f'shift_{f}_{f_max_name}_cha'] = data[f] - data[f_max_name]\n",
    "    #     data[f'shift_{f}_{f_min_name}_cha'] = data[f] - data[f_min_name]\n",
    "    #\n",
    "    #     if i == 0:\n",
    "    #         add_featuers.append(f_mean_name)\n",
    "    #         add_featuers.append(f_max_name)\n",
    "    #         add_featuers.append(f_min_name)\n",
    "    #         add_featuers.append(f_std_name)\n",
    "    #         add_featuers.append(f'shift_{f}_{f_mean_name}_cha')\n",
    "    #         add_featuers.append(f'shift_{f}_{f_max_name}_cha')\n",
    "    #         add_featuers.append(f'shift_{f}_{f_min_name}_cha')\n",
    "features.extend(add_featuers)\n",
    "\n",
    "# 对所有的特征进行划分\n",
    "for f in features:\n",
    "    if f not in ['weekday','hour', 'ts']:\n",
    "        q = len(data[f].drop_duplicates())\n",
    "        data[f] = pd.qcut(data[f], q=int(q/10), labels=False, duplicates=\"drop\")\n",
    "\n",
    "train = data[data['is_train']].reset_index(drop=True)\n",
    "test = data[~data['is_train']].reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "                  time  JS_NH3  CS_NH3  JS_TN  CS_TN  JS_LL      CS_LL  \\\n0        2022/1/4 0:00     237  0.0583    146    162   6902   996.0668   \n1        2022/1/4 0:02     237  0.0583    146    162   7154   874.0959   \n2        2022/1/4 0:04     254  0.0583    146    162   7111  1324.8105   \n3        2022/1/4 0:06     254  0.0583    146    162   6804   723.9781   \n4        2022/1/4 0:08     254  0.0583    146    162   6627   741.2993   \n...                ...     ...     ...    ...    ...    ...        ...   \n140475  2022/7/18 2:30     166  0.0169     73     69   1789  1111.9569   \n140476  2022/7/18 2:32     166  0.0169     73     69   1970  1680.4688   \n140477  2022/7/18 2:34     166  0.0169     73     69   1726  1633.5309   \n140478  2022/7/18 2:36     166  0.0169     73     69   1694  1247.8890   \n140479  2022/7/18 2:38     166  0.0169     98     69   5519   998.9305   \n\n        MCCS_NH4  MCCS_NO3  JS_COD  ...  \\\n0            113       144     117  ...   \n1            113       153     117  ...   \n2            111       148     117  ...   \n3            111       136     117  ...   \n4            111       129     117  ...   \n...          ...       ...     ...  ...   \n140475        76       253      74  ...   \n140476        75       256      74  ...   \n140477        75       255      74  ...   \n140478        75       249      74  ...   \n140479        75       250      83  ...   \n\n        N_CS_MQ_SSLL_rolling15_N_CS_MQ_SSLL_max_cha  \\\n0                                               141   \n1                                               141   \n2                                               141   \n3                                               141   \n4                                               141   \n...                                             ...   \n140475                                           31   \n140476                                           45   \n140477                                           98   \n140478                                           45   \n140479                                           27   \n\n        N_CS_MQ_SSLL_rolling15_N_CS_MQ_SSLL_min_cha  rolling15_N_QY_ORP_mean  \\\n0                                               140                       59   \n1                                               140                       59   \n2                                               140                       59   \n3                                               140                       59   \n4                                               140                       59   \n...                                             ...                      ...   \n140475                                           78                     5069   \n140476                                           23                     5048   \n140477                                           72                     5028   \n140478                                           23                     5008   \n140479                                            6                     4990   \n\n        rolling15_N_QY_ORP_max  rolling15_N_QY_ORP_min  \\\n0                            1                       2   \n1                            1                       2   \n2                            1                       2   \n3                            1                       2   \n4                            1                       2   \n...                        ...                     ...   \n140475                     111                     118   \n140476                     111                     117   \n140477                     111                     116   \n140478                     110                     116   \n140479                     110                     115   \n\n        rolling15_N_QY_ORP_corr  rolling15_N_QY_ORP_std  \\\n0                            25                       0   \n1                            25                       0   \n2                            25                       0   \n3                            25                       0   \n4                            25                       0   \n...                         ...                     ...   \n140475                     8169                    4639   \n140476                     7465                    5695   \n140477                     6949                    6514   \n140478                     6676                    7104   \n140479                     6325                    7631   \n\n        N_QY_ORP_rolling15_N_QY_ORP_mean_cha  \\\n0                                       4804   \n1                                       4804   \n2                                       4804   \n3                                       4804   \n4                                       4804   \n...                                      ...   \n140475                                   973   \n140476                                   703   \n140477                                   608   \n140478                                   533   \n140479                                   501   \n\n        N_QY_ORP_rolling15_N_QY_ORP_max_cha  \\\n0                                       100   \n1                                       100   \n2                                       100   \n3                                       100   \n4                                       100   \n...                                     ...   \n140475                                   49   \n140476                                   39   \n140477                                   35   \n140478                                   32   \n140479                                   29   \n\n        N_QY_ORP_rolling15_N_QY_ORP_min_cha  \n0                                        86  \n1                                        86  \n2                                        86  \n3                                        86  \n4                                        86  \n...                                     ...  \n140475                                    0  \n140476                                    0  \n140477                                    0  \n140478                                    0  \n140479                                    0  \n\n[140480 rows x 175 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>JS_NH3</th>\n      <th>CS_NH3</th>\n      <th>JS_TN</th>\n      <th>CS_TN</th>\n      <th>JS_LL</th>\n      <th>CS_LL</th>\n      <th>MCCS_NH4</th>\n      <th>MCCS_NO3</th>\n      <th>JS_COD</th>\n      <th>...</th>\n      <th>N_CS_MQ_SSLL_rolling15_N_CS_MQ_SSLL_max_cha</th>\n      <th>N_CS_MQ_SSLL_rolling15_N_CS_MQ_SSLL_min_cha</th>\n      <th>rolling15_N_QY_ORP_mean</th>\n      <th>rolling15_N_QY_ORP_max</th>\n      <th>rolling15_N_QY_ORP_min</th>\n      <th>rolling15_N_QY_ORP_corr</th>\n      <th>rolling15_N_QY_ORP_std</th>\n      <th>N_QY_ORP_rolling15_N_QY_ORP_mean_cha</th>\n      <th>N_QY_ORP_rolling15_N_QY_ORP_max_cha</th>\n      <th>N_QY_ORP_rolling15_N_QY_ORP_min_cha</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2022/1/4 0:00</td>\n      <td>237</td>\n      <td>0.0583</td>\n      <td>146</td>\n      <td>162</td>\n      <td>6902</td>\n      <td>996.0668</td>\n      <td>113</td>\n      <td>144</td>\n      <td>117</td>\n      <td>...</td>\n      <td>141</td>\n      <td>140</td>\n      <td>59</td>\n      <td>1</td>\n      <td>2</td>\n      <td>25</td>\n      <td>0</td>\n      <td>4804</td>\n      <td>100</td>\n      <td>86</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2022/1/4 0:02</td>\n      <td>237</td>\n      <td>0.0583</td>\n      <td>146</td>\n      <td>162</td>\n      <td>7154</td>\n      <td>874.0959</td>\n      <td>113</td>\n      <td>153</td>\n      <td>117</td>\n      <td>...</td>\n      <td>141</td>\n      <td>140</td>\n      <td>59</td>\n      <td>1</td>\n      <td>2</td>\n      <td>25</td>\n      <td>0</td>\n      <td>4804</td>\n      <td>100</td>\n      <td>86</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2022/1/4 0:04</td>\n      <td>254</td>\n      <td>0.0583</td>\n      <td>146</td>\n      <td>162</td>\n      <td>7111</td>\n      <td>1324.8105</td>\n      <td>111</td>\n      <td>148</td>\n      <td>117</td>\n      <td>...</td>\n      <td>141</td>\n      <td>140</td>\n      <td>59</td>\n      <td>1</td>\n      <td>2</td>\n      <td>25</td>\n      <td>0</td>\n      <td>4804</td>\n      <td>100</td>\n      <td>86</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2022/1/4 0:06</td>\n      <td>254</td>\n      <td>0.0583</td>\n      <td>146</td>\n      <td>162</td>\n      <td>6804</td>\n      <td>723.9781</td>\n      <td>111</td>\n      <td>136</td>\n      <td>117</td>\n      <td>...</td>\n      <td>141</td>\n      <td>140</td>\n      <td>59</td>\n      <td>1</td>\n      <td>2</td>\n      <td>25</td>\n      <td>0</td>\n      <td>4804</td>\n      <td>100</td>\n      <td>86</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2022/1/4 0:08</td>\n      <td>254</td>\n      <td>0.0583</td>\n      <td>146</td>\n      <td>162</td>\n      <td>6627</td>\n      <td>741.2993</td>\n      <td>111</td>\n      <td>129</td>\n      <td>117</td>\n      <td>...</td>\n      <td>141</td>\n      <td>140</td>\n      <td>59</td>\n      <td>1</td>\n      <td>2</td>\n      <td>25</td>\n      <td>0</td>\n      <td>4804</td>\n      <td>100</td>\n      <td>86</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>140475</th>\n      <td>2022/7/18 2:30</td>\n      <td>166</td>\n      <td>0.0169</td>\n      <td>73</td>\n      <td>69</td>\n      <td>1789</td>\n      <td>1111.9569</td>\n      <td>76</td>\n      <td>253</td>\n      <td>74</td>\n      <td>...</td>\n      <td>31</td>\n      <td>78</td>\n      <td>5069</td>\n      <td>111</td>\n      <td>118</td>\n      <td>8169</td>\n      <td>4639</td>\n      <td>973</td>\n      <td>49</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>140476</th>\n      <td>2022/7/18 2:32</td>\n      <td>166</td>\n      <td>0.0169</td>\n      <td>73</td>\n      <td>69</td>\n      <td>1970</td>\n      <td>1680.4688</td>\n      <td>75</td>\n      <td>256</td>\n      <td>74</td>\n      <td>...</td>\n      <td>45</td>\n      <td>23</td>\n      <td>5048</td>\n      <td>111</td>\n      <td>117</td>\n      <td>7465</td>\n      <td>5695</td>\n      <td>703</td>\n      <td>39</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>140477</th>\n      <td>2022/7/18 2:34</td>\n      <td>166</td>\n      <td>0.0169</td>\n      <td>73</td>\n      <td>69</td>\n      <td>1726</td>\n      <td>1633.5309</td>\n      <td>75</td>\n      <td>255</td>\n      <td>74</td>\n      <td>...</td>\n      <td>98</td>\n      <td>72</td>\n      <td>5028</td>\n      <td>111</td>\n      <td>116</td>\n      <td>6949</td>\n      <td>6514</td>\n      <td>608</td>\n      <td>35</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>140478</th>\n      <td>2022/7/18 2:36</td>\n      <td>166</td>\n      <td>0.0169</td>\n      <td>73</td>\n      <td>69</td>\n      <td>1694</td>\n      <td>1247.8890</td>\n      <td>75</td>\n      <td>249</td>\n      <td>74</td>\n      <td>...</td>\n      <td>45</td>\n      <td>23</td>\n      <td>5008</td>\n      <td>110</td>\n      <td>116</td>\n      <td>6676</td>\n      <td>7104</td>\n      <td>533</td>\n      <td>32</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>140479</th>\n      <td>2022/7/18 2:38</td>\n      <td>166</td>\n      <td>0.0169</td>\n      <td>98</td>\n      <td>69</td>\n      <td>5519</td>\n      <td>998.9305</td>\n      <td>75</td>\n      <td>250</td>\n      <td>83</td>\n      <td>...</td>\n      <td>27</td>\n      <td>6</td>\n      <td>4990</td>\n      <td>110</td>\n      <td>115</td>\n      <td>6325</td>\n      <td>7631</td>\n      <td>501</td>\n      <td>29</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>140480 rows × 175 columns</p>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 删除分布不均衡的特征"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "train = train.dropna(subset=['Label1', 'Label2']).reset_index(drop=True)\n",
    "def transform(x: pd.Series, c=20):\n",
    "    return np.log1p(x/c)\n",
    "\n",
    "def inverse_transform(x: pd.Series, c = 20):\n",
    "    return np.expm1(x)*c\n",
    "\n",
    "train = train.dropna(subset=['Label1', 'Label2']).reset_index(drop=True)\n",
    "train['Label1'] = transform(train['Label1'])\n",
    "label_c = 8\n",
    "train['Label2'] = transform(train['Label2'], c=label_c)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35068\n",
      "8.106907231653508 3.731088486252612\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.01, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.01\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\ttraining's l2: 0.0074513\tvalid_1's l2: 0.0102004\n",
      "[400]\ttraining's l2: 0.00553489\tvalid_1's l2: 0.0099458\n",
      "[600]\ttraining's l2: 0.00441363\tvalid_1's l2: 0.010009\n",
      "Early stopping, best iteration is:\n",
      "[477]\ttraining's l2: 0.00504996\tvalid_1's l2: 0.00991754\n",
      "score_list = [1299.6478409581484]\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.01, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.01\n",
      "35068\n",
      "8.729723218012834 4.510461075505404\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.01, min_child_weight=1 will be ignored. Current value: min_sum_hessian_in_leaf=0.01\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\ttraining's l2: 0.0161743\tvalid_1's l2: 0.0127661\n",
      "[400]\ttraining's l2: 0.0112628\tvalid_1's l2: 0.0107524\n",
      "[600]\ttraining's l2: 0.00983256\tvalid_1's l2: 0.0101055\n",
      "[800]\ttraining's l2: 0.009058\tvalid_1's l2: 0.00969073\n",
      "[1000]\ttraining's l2: 0.00852297\tvalid_1's l2: 0.00947869\n",
      "[1200]\ttraining's l2: 0.00807608\tvalid_1's l2: 0.00940698\n",
      "[1400]\ttraining's l2: 0.00770004\tvalid_1's l2: 0.0093837\n",
      "[1600]\ttraining's l2: 0.00735111\tvalid_1's l2: 0.00937063\n",
      "Early stopping, best iteration is:\n",
      "[1639]\ttraining's l2: 0.00729306\tvalid_1's l2: 0.00936482\n",
      "score_list = [1299.6478409581484, 1095.4071904570837]\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.01, min_child_weight=1 will be ignored. Current value: min_sum_hessian_in_leaf=0.01\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "score_list = []\n",
    "for label in labels:\n",
    "    train[label] = train[label].fillna(0)\n",
    "    tmp = train[~train[label].isna()].reset_index(drop=True).copy(deep=True)\n",
    "    print(len(tmp))\n",
    "    print(np.max(tmp[label]), np.min(tmp[label]))\n",
    "    test_size = 4000\n",
    "    X_train = tmp[features][:-test_size]\n",
    "    X_test = tmp[features][-test_size:]\n",
    "    y_train = tmp[label][:-test_size]\n",
    "    y_test = tmp[label][-test_size:]\n",
    "    if label == \"Label1\":\n",
    "        model = lgb.LGBMRegressor(\n",
    "            boosting=\"gbdt\",\n",
    "            max_depth=4,\n",
    "            learning_rate=0.1,\n",
    "            n_estimators=10000,\n",
    "            min_data_in_leaf=50,\n",
    "            subsample = 0.8,\n",
    "            feature_fraction=0.3,\n",
    "            bagging_seed=1,\n",
    "            reg_alpha=1,\n",
    "            reg_lambda=1,  # 此处不改了\n",
    "            min_sum_hessian_in_leaf=0.01,\n",
    "            random_state=1212\n",
    "        )\n",
    "        model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], eval_metric=['mse'],\n",
    "              early_stopping_rounds=150, verbose=200)\n",
    "        test_pred = model.predict(X_test)\n",
    "        score_list.append(np.sqrt(mean_squared_error(inverse_transform(y_test), inverse_transform(test_pred))))\n",
    "        print(f\"score_list = {score_list}\")\n",
    "        model2 = lgb.LGBMRegressor(\n",
    "            boosting=\"gbdt\",\n",
    "            max_depth=4,\n",
    "            learning_rate=0.1,\n",
    "            n_estimators=int(1.2*model.best_iteration_),\n",
    "            min_data_in_leaf=50,\n",
    "            subsample = 0.8,\n",
    "            feature_fraction=0.3,\n",
    "            bagging_seed=1,\n",
    "            reg_alpha=1,\n",
    "            reg_lambda=1,  # 此处不改了\n",
    "            min_sum_hessian_in_leaf=0.01,\n",
    "            random_state=1212\n",
    "        )\n",
    "        model2.fit(tmp[features], tmp[label])\n",
    "        test[label] = inverse_transform(model2.predict(test[features]))\n",
    "    else:\n",
    "        model = lgb.LGBMRegressor(\n",
    "            boosting=\"gbdt\",\n",
    "            max_depth=4,\n",
    "            learning_rate=0.01,\n",
    "            n_estimators=10000,\n",
    "            min_child_weight=1,\n",
    "            min_data_in_leaf=40,\n",
    "            subsample = 0.4,\n",
    "            feature_fraction=0.3,\n",
    "            bagging_seed=11,\n",
    "            reg_alpha=1,\n",
    "            reg_lambda=1,  # 此处不改了\n",
    "            min_sum_hessian_in_leaf=0.01,\n",
    "            random_state=222\n",
    "        )\n",
    "\n",
    "        model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], eval_metric=['mse'],\n",
    "              early_stopping_rounds=150, verbose=200)\n",
    "        test_pred = model.predict(X_test)\n",
    "        score_list.append(np.sqrt(mean_squared_error(inverse_transform(y_test, c=label_c), inverse_transform(test_pred, c=label_c))))\n",
    "        print(f\"score_list = {score_list}\")\n",
    "        model2 = lgb.LGBMRegressor(\n",
    "            boosting=\"gbdt\",\n",
    "            max_depth=4,\n",
    "            learning_rate=0.01,\n",
    "            n_estimators=int(1.2*model.best_iteration_),\n",
    "            min_child_weight=1,\n",
    "            min_data_in_leaf=40,\n",
    "            subsample = 0.4,\n",
    "            feature_fraction=0.3,\n",
    "            bagging_seed=11,\n",
    "            reg_alpha=1,\n",
    "            reg_lambda=1,  # 此处不改了\n",
    "            min_sum_hessian_in_leaf=0.01,\n",
    "            random_state=222\n",
    "        )\n",
    "        model2.fit(tmp[features], tmp[label])\n",
    "\n",
    "        test[label] = inverse_transform(model2.predict(test[features]), c=label_c)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8343571481624228\n"
     ]
    }
   ],
   "source": [
    "loss = np.mean(score_list)\n",
    "score = 1000/(1+loss)\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "test[['time'] + labels].to_csv(f\"./res/lightgbm_res.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}