{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/train_dataset.csv\")\n",
    "\n",
    "data['is_train'] = True\n",
    "evaluation = pd.read_csv(\"./data/evaluation_public.csv\")\n",
    "evaluation['is_train'] = False\n",
    "sample = pd.read_csv(\"./data/sample_submission.csv\")\n",
    "\n",
    "data = pd.concat([data, evaluation]).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 特征处理"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "data['date'] = pd.to_datetime(data['time'])\n",
    "data['hour'] = data['date'].dt.hour\n",
    "data['year'] = data['date'].dt.year\n",
    "data['month'] = data['date'].dt.month\n",
    "data['minute'] = data['date'].dt.minute\n",
    "data['weekday'] = data['date'].dt.weekday\n",
    "data['day'] = data['date'].dt.day\n",
    "data['hour'] = data['date'].dt.hour\n",
    "# data['ts'] = data['hour']*3600 + data['minute']*60 + data['date'].dt.second\n",
    "data['ts'] = data['hour']*60 + data['minute']\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import gc\n",
    "features = [ 'JS_NH3', 'CS_NH3', 'JS_TN', 'CS_TN', 'JS_LL', 'CS_LL', 'MCCS_NH4', 'MCCS_NO3', 'JS_COD', 'CS_COD', 'JS_SW', 'CS_SW', 'B_HYC_NH4', 'B_HYC_XD', 'B_HYC_MLSS', 'B_HYC_JS_DO', 'B_HYC_DO', 'B_CS_MQ_SSLL', 'B_QY_ORP', 'N_HYC_NH4', 'N_HYC_XD', 'N_HYC_MLSS', 'N_HYC_JS_DO', 'N_HYC_DO', 'N_CS_MQ_SSLL', 'N_QY_ORP','weekday','hour', 'ts']\n",
    "\n",
    "filter_set = {'time', 'Label1', 'Label2','CS_LL','CS_NH3', 'JS_SW', 'B_QY_ORP'}\n",
    "features = [f for f in features if f not in filter_set] #\n",
    "for f in features:\n",
    "    data[f] = data[f].fillna(method='ffill')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 提取特征的diff值"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_4584/1814523910.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_mean_name}_radio'] = data[f'{f}_{f_min_name}_cha']/data[f]\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_4584/1814523910.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_diff'] = data[f].diff()\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_4584/1814523910.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_mean_name] = train_rolling.mean().fillna(0).values\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_4584/1814523910.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_max_name] = train_rolling.max().fillna(0).values\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_4584/1814523910.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_min_name] = train_rolling.min().fillna(0).values\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_4584/1814523910.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_std_name] = train_rolling.std().fillna(0).values\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_4584/1814523910.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_corr_name] = train_rolling.corr().fillna(0).values\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_4584/1814523910.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_skew_name] = train_rolling.skew().fillna(0).values\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_4584/1814523910.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_mean_name}_cha'] = data[f] - data[f_mean_name]\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_4584/1814523910.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_max_name}_cha'] = data[f] - data[f_max_name]\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_4584/1814523910.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_min_name}_cha'] = data[f] - data[f_min_name]\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_4584/1814523910.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_min_name}_radio'] = data[f'{f}_{f_mean_name}_cha']/data[f]\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_4584/1814523910.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_max_name}_radio'] = data[f'{f}_{f_max_name}_cha']/data[f]\n"
     ]
    }
   ],
   "source": [
    "df_list = []\n",
    "i = 0\n",
    "add_featuers = []\n",
    "length = 0\n",
    "# JS_TN, MCCS_NO3,MCCS_NH4,'B_QY_ORP'\n",
    "filter_set = {'weekday', 'hour', 'ts', 'B_QY_ORP', 'JS_TN', 'CS_SW', 'MCCS_NH4', 'N_HYC_JS_DO', 'MCCS_NO3', 'JS_SW'}\n",
    "# filter_set = {'weekday', 'hour', 'ts', 'MCCS_NO3'}\n",
    "for f in features:\n",
    "    if f in filter_set: continue\n",
    "    for r in [15]:\n",
    "        train_rolling = data[f].rolling(window=r, center=False)\n",
    "        f_mean_name = 'rolling{}_{}_mean'.format(r,f)\n",
    "        f_max_name = 'rolling{}_{}_max'.format(r,f)\n",
    "        f_min_name = 'rolling{}_{}_min'.format(r,f)\n",
    "        f_std_name = 'rolling{}_{}_std'.format(r,f)\n",
    "        f_corr_name = 'rolling{}_{}_corr'.format(r,f)\n",
    "        f_cov_name = 'rolling{}_{}_cov'.format(r,f)\n",
    "        f_skew_name = 'rolling{}_{}_skew'.format(r,f)\n",
    "        f_kurt_name = 'rolling{}_{}_kurt'.format(r,f)\n",
    "        data[f_mean_name] = train_rolling.mean().fillna(0).values\n",
    "        data[f_max_name] = train_rolling.max().fillna(0).values\n",
    "        data[f_min_name] = train_rolling.min().fillna(0).values\n",
    "        data[f_std_name] = train_rolling.std().fillna(0).values\n",
    "        data[f_corr_name] = train_rolling.corr().fillna(0).values\n",
    "        data[f_skew_name] = train_rolling.skew().fillna(0).values\n",
    "\n",
    "        data[f'{f}_{f_mean_name}_cha'] = data[f] - data[f_mean_name]\n",
    "        data[f'{f}_{f_max_name}_cha'] = data[f] - data[f_max_name]\n",
    "        data[f'{f}_{f_min_name}_cha'] = data[f] - data[f_min_name]\n",
    "\n",
    "        data[f'{f}_{f_min_name}_radio'] = data[f'{f}_{f_mean_name}_cha']/data[f]\n",
    "        data[f'{f}_{f_max_name}_radio'] = data[f'{f}_{f_max_name}_cha']/data[f]\n",
    "        data[f'{f}_{f_mean_name}_radio'] = data[f'{f}_{f_min_name}_cha']/data[f]\n",
    "        data[f'{f}_diff'] = data[f].diff()\n",
    "\n",
    "        if i == 0:\n",
    "            add_featuers.append(f_mean_name)\n",
    "            add_featuers.append(f_max_name)\n",
    "            add_featuers.append(f_min_name)\n",
    "            add_featuers.append(f_std_name)\n",
    "            add_featuers.append(f_corr_name)\n",
    "            add_featuers.append(f_skew_name)\n",
    "            add_featuers.append(f'{f}_{f_mean_name}_cha')\n",
    "            add_featuers.append(f'{f}_{f_max_name}_cha')\n",
    "            add_featuers.append(f'{f}_{f_min_name}_cha')\n",
    "            add_featuers.append(f'{f}_diff')\n",
    "\n",
    "features.extend(add_featuers)\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "# 对所有的特征进行划分\n",
    "for f in features:\n",
    "    if f not in ['weekday','hour', 'ts']:\n",
    "        q = len(data[f].drop_duplicates())\n",
    "\n",
    "        data[f] = pd.qcut(data[f], q=int(q/10), labels=False, duplicates=\"drop\")\n",
    "\n",
    "all_train = data[data['is_train']].reset_index(drop=True)\n",
    "test = data[~data['is_train']].reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 删除分布不均衡的特征"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "all_train = all_train.dropna(subset=['Label1', 'Label2']).reset_index(drop=True)\n",
    "def transform(x: pd.Series, c=20):\n",
    "    return np.log1p(x/c)\n",
    "\n",
    "def inverse_transform(x: pd.Series, c = 20):\n",
    "    return np.expm1(x)*c\n",
    "\n",
    "all_train = all_train.dropna(subset=['Label1', 'Label2']).reset_index(drop=True)\n",
    "all_train['Label1'] = transform(all_train['Label1'])\n",
    "label_c = 8\n",
    "all_train['Label2'] = transform(all_train['Label2'], c=label_c)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot(y1, y2, x_title, y_title, title):\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.plot(range(len(y1)), y1)\n",
    "    plt.plot(range(len(y1)), y2)\n",
    "    plt.legend()\n",
    "    plt.xlabel(x_title)\n",
    "    plt.ylabel(y_title)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Custom logger is already specified. Specify more than one logger at same time is not thread safe."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.2910019\ttest: 0.2910019\ttest1: 0.1773276\tbest: 0.1773276 (0)\ttotal: 8.35ms\tremaining: 6m 57s\n",
      "200:\tlearn: 0.0947468\ttest: 0.0947468\ttest1: 0.1029159\tbest: 0.1028754 (198)\ttotal: 2.49s\tremaining: 10m 17s\n",
      "400:\tlearn: 0.0782055\ttest: 0.0782055\ttest1: 0.1001376\tbest: 0.1000350 (354)\ttotal: 4.64s\tremaining: 9m 34s\n",
      "600:\tlearn: 0.0679818\ttest: 0.0679818\ttest1: 0.0985185\tbest: 0.0984667 (565)\ttotal: 6.82s\tremaining: 9m 20s\n",
      "800:\tlearn: 0.0610586\ttest: 0.0610586\ttest1: 0.0979228\tbest: 0.0979123 (796)\ttotal: 9s\tremaining: 9m 13s\n",
      "1000:\tlearn: 0.0559841\ttest: 0.0559841\ttest1: 0.0977013\tbest: 0.0976694 (958)\ttotal: 11.2s\tremaining: 9m 5s\n",
      "1200:\tlearn: 0.0521056\ttest: 0.0521056\ttest1: 0.0973847\tbest: 0.0973805 (1197)\ttotal: 13.3s\tremaining: 8m 58s\n",
      "1400:\tlearn: 0.0489566\ttest: 0.0489566\ttest1: 0.0971754\tbest: 0.0971282 (1356)\ttotal: 15.4s\tremaining: 8m 55s\n",
      "1600:\tlearn: 0.0462625\ttest: 0.0462625\ttest1: 0.0971614\tbest: 0.0971052 (1463)\ttotal: 17.6s\tremaining: 8m 50s\n",
      "Stopped by overfitting detector  (150 iterations wait)\n",
      "\n",
      "bestTest = 0.09710515234\n",
      "bestIteration = 1463\n",
      "\n",
      "Shrink model to first 1464 iterations.\n",
      "score_list1 = [1264.1029074427938]\n",
      "0:\tlearn: 0.2802931\ttotal: 9.31ms\tremaining: 16.3s\n",
      "200:\tlearn: 0.0947288\ttotal: 2.14s\tremaining: 16.5s\n",
      "400:\tlearn: 0.0795068\ttotal: 4.38s\tremaining: 14.8s\n",
      "600:\tlearn: 0.0702771\ttotal: 6.91s\tremaining: 13.3s\n",
      "800:\tlearn: 0.0641771\ttotal: 9.3s\tremaining: 11.1s\n",
      "1000:\tlearn: 0.0596187\ttotal: 11.4s\tremaining: 8.59s\n",
      "1200:\tlearn: 0.0560273\ttotal: 13.5s\tremaining: 6.23s\n",
      "1400:\tlearn: 0.0531502\ttotal: 15.6s\tremaining: 3.94s\n",
      "1600:\tlearn: 0.0506714\ttotal: 17.7s\tremaining: 1.71s\n",
      "1754:\tlearn: 0.0490343\ttotal: 19.4s\tremaining: 0us\n",
      "0:\tlearn: 0.2733858\ttest: 0.2733858\ttest1: 0.1806163\tbest: 0.1806163 (0)\ttotal: 10.7ms\tremaining: 8m 52s\n",
      "200:\tlearn: 0.0944824\ttest: 0.0944824\ttest1: 0.0998865\tbest: 0.0998786 (199)\ttotal: 2.33s\tremaining: 9m 36s\n",
      "400:\tlearn: 0.0793597\ttest: 0.0793597\ttest1: 0.0956439\tbest: 0.0956439 (400)\ttotal: 4.65s\tremaining: 9m 35s\n",
      "600:\tlearn: 0.0691262\ttest: 0.0691262\ttest1: 0.0947486\tbest: 0.0947330 (548)\ttotal: 6.89s\tremaining: 9m 26s\n",
      "800:\tlearn: 0.0626738\ttest: 0.0626738\ttest1: 0.0942079\tbest: 0.0941521 (768)\ttotal: 9.15s\tremaining: 9m 22s\n",
      "Stopped by overfitting detector  (150 iterations wait)\n",
      "\n",
      "bestTest = 0.09404656782\n",
      "bestIteration = 848\n",
      "\n",
      "Shrink model to first 849 iterations.\n",
      "score_list2 = [1264.1029074427938, 1055.5455580358382]\n",
      "0:\tlearn: 0.2641369\ttotal: 20.5ms\tremaining: 20.9s\n",
      "200:\tlearn: 0.0944732\ttotal: 2.86s\tremaining: 11.6s\n",
      "400:\tlearn: 0.0806393\ttotal: 5.52s\tremaining: 8.48s\n",
      "600:\tlearn: 0.0714837\ttotal: 8.36s\tremaining: 5.79s\n",
      "800:\tlearn: 0.0654535\ttotal: 11s\tremaining: 2.96s\n",
      "1000:\tlearn: 0.0611814\ttotal: 13.8s\tremaining: 221ms\n",
      "1016:\tlearn: 0.0608839\ttotal: 14s\tremaining: 0us\n",
      "0.8614568612512487\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import catboost as cbt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "labels = ['Label1', 'Label2']\n",
    "score_list = []\n",
    "test_size = 4000\n",
    "\n",
    "for label in labels:\n",
    "    X_train = all_train[features][:-test_size]\n",
    "    y_train = all_train[label][:-test_size]\n",
    "\n",
    "    X_test = all_train[features][-test_size:]\n",
    "    y_test = all_train[label][-test_size:]\n",
    "    if label == \"Label1\":\n",
    "        model = cbt.CatBoostRegressor(\n",
    "            depth=7,\n",
    "            learning_rate=0.035,\n",
    "            l2_leaf_reg=2,\n",
    "            early_stopping_rounds=100,\n",
    "            num_trees=50000,\n",
    "            min_data_in_leaf=10,\n",
    "            subsample = 0.5,\n",
    "            colsample_bylevel  = 0.2,\n",
    "            border_count=128,\n",
    "            random_seed=1212\n",
    "        )\n",
    "        model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "              early_stopping_rounds=150, verbose=200)\n",
    "        test_pred = model.predict(X_test)\n",
    "\n",
    "        score_list.append(np.sqrt(mean_squared_error(inverse_transform(y_test), inverse_transform(test_pred))))\n",
    "        print(f\"score_list1 = {score_list}\")\n",
    "        #\n",
    "        model2 = cbt.CatBoostRegressor(\n",
    "            depth=7,\n",
    "            learning_rate=0.035,\n",
    "            l2_leaf_reg=2,\n",
    "            early_stopping_rounds=100,\n",
    "            num_trees=int(1.2*model.best_iteration_),\n",
    "            min_data_in_leaf=10,\n",
    "            subsample = 0.5,\n",
    "            colsample_bylevel  = 0.2,\n",
    "            border_count=128,\n",
    "            random_seed=1212\n",
    "        )\n",
    "        model2.fit(all_train[features], all_train[label], verbose=200)\n",
    "        test[label] = inverse_transform(model2.predict(test[features]))\n",
    "    else:\n",
    "        model = cbt.CatBoostRegressor(\n",
    "            depth=7,\n",
    "            learning_rate=0.03,\n",
    "            l2_leaf_reg=2,\n",
    "            early_stopping_rounds=100,\n",
    "            num_trees=50000,\n",
    "            min_data_in_leaf=10,\n",
    "            subsample = 0.4,\n",
    "            colsample_bylevel  = 0.3,\n",
    "            border_count=128,\n",
    "            random_seed=1212\n",
    "        )\n",
    "\n",
    "        model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "              early_stopping_rounds=150, verbose=200)\n",
    "        test_pred = model.predict(X_test)\n",
    "        score_list.append(np.sqrt(mean_squared_error(inverse_transform(y_test, c=label_c), inverse_transform(test_pred, c=label_c))))\n",
    "        print(f\"score_list2 = {score_list}\")\n",
    "        #\n",
    "        model2 = cbt.CatBoostRegressor(\n",
    "            depth=7,\n",
    "            learning_rate=0.03,\n",
    "            l2_leaf_reg=2,\n",
    "            early_stopping_rounds=100,\n",
    "            num_trees=int(1.2*model.best_iteration_),\n",
    "            min_data_in_leaf=10,\n",
    "            subsample = 0.4,\n",
    "            colsample_bylevel  = 0.3,\n",
    "            border_count=128,\n",
    "            random_seed=1212\n",
    "        )\n",
    "        model2.fit(all_train[features], all_train[label], verbose=200)\n",
    "        #\n",
    "        test[label] = inverse_transform(model2.predict(test[features]), c=label_c)\n",
    "loss = np.mean(score_list)\n",
    "score = 1000/(1+loss)\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8614568612512487\n"
     ]
    }
   ],
   "source": [
    "loss = np.mean(score_list)\n",
    "score = 1000/(1+loss)\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "test[['time'] + labels].to_csv(f\"./res/catboost_res.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8614568612512487\n"
     ]
    }
   ],
   "source": [
    "loss = np.mean(score_list)\n",
    "score = 1000/(1+loss)\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}