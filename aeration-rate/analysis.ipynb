{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/train_dataset.csv\")\n",
    "\n",
    "data['is_test'] = False\n",
    "evaluation = pd.read_csv(\"./data/evaluation_public.csv\")\n",
    "evaluation['is_test'] = True\n",
    "sample = pd.read_csv(\"./data/sample_submission.csv\")\n",
    "\n",
    "all_data = pd.concat([data, evaluation]).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 140480 entries, 0 to 140479\n",
      "Data columns (total 30 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   time          140480 non-null  object \n",
      " 1   JS_NH3        113979 non-null  float64\n",
      " 2   CS_NH3        113493 non-null  float64\n",
      " 3   JS_TN         113979 non-null  float64\n",
      " 4   CS_TN         113593 non-null  float64\n",
      " 5   JS_LL         120761 non-null  float64\n",
      " 6   CS_LL         120971 non-null  float64\n",
      " 7   MCCS_NH4      120974 non-null  float64\n",
      " 8   MCCS_NO3      120974 non-null  float64\n",
      " 9   JS_COD        113979 non-null  float64\n",
      " 10  CS_COD        120595 non-null  float64\n",
      " 11  JS_SW         112368 non-null  float64\n",
      " 12  CS_SW         112587 non-null  float64\n",
      " 13  B_HYC_NH4     120975 non-null  float64\n",
      " 14  B_HYC_XD      120975 non-null  float64\n",
      " 15  B_HYC_MLSS    120975 non-null  float64\n",
      " 16  B_HYC_JS_DO   120975 non-null  float64\n",
      " 17  B_HYC_DO      120975 non-null  float64\n",
      " 18  B_CS_MQ_SSLL  121595 non-null  float64\n",
      " 19  B_QY_ORP      120975 non-null  float64\n",
      " 20  N_HYC_NH4     120975 non-null  float64\n",
      " 21  N_HYC_XD      120975 non-null  float64\n",
      " 22  N_HYC_MLSS    120975 non-null  float64\n",
      " 23  N_HYC_JS_DO   120974 non-null  float64\n",
      " 24  N_HYC_DO      120975 non-null  float64\n",
      " 25  N_CS_MQ_SSLL  121595 non-null  float64\n",
      " 26  N_QY_ORP      120975 non-null  float64\n",
      " 27  Label1        35068 non-null   float64\n",
      " 28  Label2        35068 non-null   float64\n",
      " 29  is_test       140480 non-null  bool   \n",
      "dtypes: bool(1), float64(28), object(1)\n",
      "memory usage: 31.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 特征处理"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "all_data['date'] = pd.to_datetime(all_data['time'])\n",
    "all_data['hour'] = all_data['date'].dt.hour\n",
    "all_data['year'] = all_data['date'].dt.year\n",
    "all_data['month'] = all_data['date'].dt.month\n",
    "all_data['minute'] = all_data['date'].dt.minute\n",
    "all_data['weekday'] = all_data['date'].dt.weekday\n",
    "all_data['day'] = all_data['date'].dt.day\n",
    "all_data['hour'] = all_data['date'].dt.hour\n",
    "all_data['ts'] = all_data['hour']*3600 + all_data['minute']*60 + all_data['date'].dt.second"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import gc\n",
    "features = [ 'JS_NH3', 'CS_NH3', 'JS_TN', 'CS_TN', 'JS_LL', 'CS_LL', 'MCCS_NH4', 'MCCS_NO3', 'JS_COD', 'CS_COD', 'JS_SW', 'CS_SW', 'B_HYC_NH4', 'B_HYC_XD', 'B_HYC_MLSS', 'B_HYC_JS_DO', 'B_HYC_DO', 'B_CS_MQ_SSLL', 'B_QY_ORP', 'N_HYC_NH4', 'N_HYC_XD', 'N_HYC_MLSS', 'N_HYC_JS_DO', 'N_HYC_DO', 'N_CS_MQ_SSLL', 'N_QY_ORP','weekday','hour', 'ts']\n",
    "features = [f for f in features if f not in ['time', 'Label1', 'Label2','CS_LL','CS_NH3', 'JS_SW']] #\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "labels = ['Label1', 'Label2']\n",
    "train = all_data[~all_data['is_test']].copy(deep=True)\n",
    "test = all_data[all_data['is_test']].copy(deep=True)\n",
    "test['is_train'] = False\n",
    "train['is_train'] = True\n",
    "data = pd.concat([train, test]).reset_index(drop=True)\n",
    "del all_data\n",
    "gc.collect()\n",
    "\n",
    "for f in features:\n",
    "    data[f] = data[f].fillna(method='ffill')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 提取特征的diff值"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# tmp_features = list(features)\n",
    "# add_dict = {}\n",
    "# for f in tmp_features:\n",
    "#     data[f'{f}_diff']  = data[f].shift()\n",
    "#     features.append(f'{f}_diff')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# fff = ['MCCS_NO3', 'B_HYC_NH4','N_HYC_NH4']\n",
    "# for f in features:\n",
    "#     data[f] = data[f].fillna(method='ffill')\n",
    "# for i in range(len(fff)-1):\n",
    "#     for j in range(i+1, len(fff)):\n",
    "#         f1 = features[i]\n",
    "#         f2 = features[j]\n",
    "#         # feature_jian = f\"{f1}_{f2}_jian\"\n",
    "#         feature_radio = f\"{f1}_{f2}_radio\"\n",
    "#         data[feature_radio] = data[f1]/data[f2]\n",
    "#         # data[feature_jian] = data[f1]-data[f2]\n",
    "#         features.append(feature_radio)\n",
    "#         # features.append(feature_jian)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_mean_name}_cha'] = data[f] - data[f_mean_name]\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_max_name}_cha'] = data[f] - data[f_max_name]\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_min_name}_cha'] = data[f] - data[f_min_name]\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_mean_name] = train_rolling.mean().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_max_name] = train_rolling.max().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_min_name] = train_rolling.min().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_std_name] = train_rolling.corr().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_corr_name] = train_rolling.std().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_mean_name}_cha'] = data[f] - data[f_mean_name]\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_max_name}_cha'] = data[f] - data[f_max_name]\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_min_name}_cha'] = data[f] - data[f_min_name]\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_mean_name] = train_rolling.mean().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_max_name] = train_rolling.max().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_min_name] = train_rolling.min().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_std_name] = train_rolling.corr().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_corr_name] = train_rolling.std().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_mean_name}_cha'] = data[f] - data[f_mean_name]\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_max_name}_cha'] = data[f] - data[f_max_name]\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_min_name}_cha'] = data[f] - data[f_min_name]\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_mean_name] = train_rolling.mean().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_max_name] = train_rolling.max().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_min_name] = train_rolling.min().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_std_name] = train_rolling.corr().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_corr_name] = train_rolling.std().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_mean_name}_cha'] = data[f] - data[f_mean_name]\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_max_name}_cha'] = data[f] - data[f_max_name]\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_min_name}_cha'] = data[f] - data[f_min_name]\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_mean_name] = train_rolling.mean().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_max_name] = train_rolling.max().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_min_name] = train_rolling.min().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_std_name] = train_rolling.corr().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_corr_name] = train_rolling.std().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_mean_name}_cha'] = data[f] - data[f_mean_name]\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_max_name}_cha'] = data[f] - data[f_max_name]\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_min_name}_cha'] = data[f] - data[f_min_name]\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_mean_name] = train_rolling.mean().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_max_name] = train_rolling.max().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_min_name] = train_rolling.min().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_std_name] = train_rolling.corr().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_corr_name] = train_rolling.std().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_mean_name}_cha'] = data[f] - data[f_mean_name]\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_max_name}_cha'] = data[f] - data[f_max_name]\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_min_name}_cha'] = data[f] - data[f_min_name]\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_mean_name] = train_rolling.mean().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_max_name] = train_rolling.max().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_min_name] = train_rolling.min().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_std_name] = train_rolling.corr().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_corr_name] = train_rolling.std().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_mean_name}_cha'] = data[f] - data[f_mean_name]\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_max_name}_cha'] = data[f] - data[f_max_name]\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_min_name}_cha'] = data[f] - data[f_min_name]\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_mean_name] = train_rolling.mean().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_max_name] = train_rolling.max().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_min_name] = train_rolling.min().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_std_name] = train_rolling.corr().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_corr_name] = train_rolling.std().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_mean_name}_cha'] = data[f] - data[f_mean_name]\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_max_name}_cha'] = data[f] - data[f_max_name]\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_min_name}_cha'] = data[f] - data[f_min_name]\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_mean_name] = train_rolling.mean().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_max_name] = train_rolling.max().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_min_name] = train_rolling.min().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_std_name] = train_rolling.corr().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_corr_name] = train_rolling.std().fillna(0).values\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_mean_name}_cha'] = data[f] - data[f_mean_name]\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_max_name}_cha'] = data[f] - data[f_max_name]\n",
      "C:\\Users\\hyj\\AppData\\Local\\Temp\\ipykernel_4220\\2850593650.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_min_name}_cha'] = data[f] - data[f_min_name]\n",
      "C:\\Users\\hyj\\.conda\\envs\\python38\\lib\\site-packages\\numpy\\lib\\function_base.py:4527: RuntimeWarning: invalid value encountered in subtract\n",
      "  diff_b_a = subtract(b, a)\n"
     ]
    }
   ],
   "source": [
    "df_list = []\n",
    "i = 0\n",
    "add_featuers = []\n",
    "length = 0\n",
    "for f in ['JS_NH3', 'CS_TN', 'JS_LL', 'JS_COD', 'CS_COD', 'B_HYC_NH4', 'B_HYC_XD', 'B_HYC_MLSS', 'B_HYC_JS_DO', 'B_HYC_DO', 'B_CS_MQ_SSLL',\n",
    "          'N_HYC_NH4', 'N_HYC_XD', 'N_HYC_MLSS', 'N_HYC_DO', 'N_CS_MQ_SSLL', 'N_QY_ORP']:\n",
    "    for r in [15]:\n",
    "        train_rolling = data[f].rolling(window=r, center=False)\n",
    "        f_mean_name = 'rolling{}_{}_mean'.format(r,f)\n",
    "        f_max_name = 'rolling{}_{}_max'.format(r,f)\n",
    "        f_min_name = 'rolling{}_{}_min'.format(r,f)\n",
    "        f_std_name = 'rolling{}_{}_std'.format(r,f)\n",
    "        f_corr_name = 'rolling{}_{}_corr'.format(r,f)\n",
    "        data[f_mean_name] = train_rolling.mean().fillna(0).values\n",
    "        data[f_max_name] = train_rolling.max().fillna(0).values\n",
    "        data[f_min_name] = train_rolling.min().fillna(0).values\n",
    "        data[f_std_name] = train_rolling.corr().fillna(0).values\n",
    "        data[f_corr_name] = train_rolling.std().fillna(0).values\n",
    "        data[f'{f}_{f_mean_name}_cha'] = data[f] - data[f_mean_name]\n",
    "        data[f'{f}_{f_max_name}_cha'] = data[f] - data[f_max_name]\n",
    "        data[f'{f}_{f_min_name}_cha'] = data[f] - data[f_min_name]\n",
    "\n",
    "        if i == 0:\n",
    "            add_featuers.append(f_mean_name)\n",
    "            add_featuers.append(f_max_name)\n",
    "            add_featuers.append(f_min_name)\n",
    "            add_featuers.append(f_std_name)\n",
    "            add_featuers.append(f_corr_name)\n",
    "            add_featuers.append(f'{f}_{f_mean_name}_cha')\n",
    "            add_featuers.append(f'{f}_{f_max_name}_cha')\n",
    "            add_featuers.append(f'{f}_{f_min_name}_cha')\n",
    "\n",
    "    # for r in [5]:\n",
    "    #     train_rolling = data[f].shift(15).rolling(window=r, center=False)\n",
    "    #     f_mean_name = 'shift_rolling{}_{}_mean'.format(r,f)\n",
    "    #     f_max_name = 'shift_rolling{}_{}_max'.format(r,f)\n",
    "    #     f_min_name = 'shift_rolling{}_{}_min'.format(r,f)\n",
    "    #     f_std_name = 'shift_rolling{}_{}_std'.format(r,f)\n",
    "    #     data[f_mean_name] = train_rolling.mean().fillna(0).values\n",
    "    #     data[f_max_name] = train_rolling.max().fillna(0).values\n",
    "    #     data[f_min_name] = train_rolling.min().fillna(0).values\n",
    "    #     data[f_std_name] = train_rolling.std().fillna(0).values\n",
    "    #     data[f'shift_{f}_{f_mean_name}_cha'] = data[f] - data[f_mean_name]\n",
    "    #     data[f'shift_{f}_{f_max_name}_cha'] = data[f] - data[f_max_name]\n",
    "    #     data[f'shift_{f}_{f_min_name}_cha'] = data[f] - data[f_min_name]\n",
    "    #\n",
    "    #     if i == 0:\n",
    "    #         add_featuers.append(f_mean_name)\n",
    "    #         add_featuers.append(f_max_name)\n",
    "    #         add_featuers.append(f_min_name)\n",
    "    #         add_featuers.append(f_std_name)\n",
    "    #         add_featuers.append(f'shift_{f}_{f_mean_name}_cha')\n",
    "    #         add_featuers.append(f'shift_{f}_{f_max_name}_cha')\n",
    "    #         add_featuers.append(f'shift_{f}_{f_min_name}_cha')\n",
    "features.extend(add_featuers)\n",
    "\n",
    "# 对所有的特征进行划分\n",
    "for f in features:\n",
    "    if f not in ['weekday','hour', 'ts']:\n",
    "        q = len(data[f].drop_duplicates())\n",
    "        data[f] = pd.qcut(data[f], q=int(q/10), labels=False, duplicates=\"drop\")\n",
    "\n",
    "train = data[data['is_train']].reset_index(drop=True)\n",
    "test = data[~data['is_train']].reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 删除分布不均衡的特征"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "train = train.dropna(subset=['Label1', 'Label2']).reset_index(drop=True)\n",
    "def transform(x: pd.Series, c=20):\n",
    "    return np.log1p(x/c)\n",
    "\n",
    "def inverse_transform(x: pd.Series, c = 20):\n",
    "    return np.expm1(x)*c\n",
    "\n",
    "train = train.dropna(subset=['Label1', 'Label2']).reset_index(drop=True)\n",
    "train['Label1'] = transform(train['Label1'])\n",
    "label_c = 8\n",
    "train['Label2'] = transform(train['Label2'], c=label_c)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35068\n",
      "8.106907231653508 3.731088486252612\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.01, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hyj\\.conda\\envs\\python38\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\hyj\\.conda\\envs\\python38\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttraining's rmse: 0.130295\ttraining's l2: 0.0169767\tvalid_1's rmse: 0.108602\tvalid_1's l2: 0.0117944\n",
      "[400]\ttraining's rmse: 0.108607\ttraining's l2: 0.0117954\tvalid_1's rmse: 0.106446\tvalid_1's l2: 0.0113307\n",
      "[600]\ttraining's rmse: 0.101853\ttraining's l2: 0.010374\tvalid_1's rmse: 0.105489\tvalid_1's l2: 0.011128\n",
      "[800]\ttraining's rmse: 0.0977857\ttraining's l2: 0.00956204\tvalid_1's rmse: 0.10428\tvalid_1's l2: 0.0108743\n",
      "[1000]\ttraining's rmse: 0.094581\ttraining's l2: 0.00894557\tvalid_1's rmse: 0.103203\tvalid_1's l2: 0.0106508\n",
      "[1200]\ttraining's rmse: 0.0918588\ttraining's l2: 0.00843803\tvalid_1's rmse: 0.102455\tvalid_1's l2: 0.010497\n",
      "[1400]\ttraining's rmse: 0.0896064\ttraining's l2: 0.00802931\tvalid_1's rmse: 0.102002\tvalid_1's l2: 0.0104044\n",
      "[1600]\ttraining's rmse: 0.0876879\ttraining's l2: 0.00768916\tvalid_1's rmse: 0.101633\tvalid_1's l2: 0.0103293\n",
      "[1800]\ttraining's rmse: 0.0859378\ttraining's l2: 0.00738531\tvalid_1's rmse: 0.101295\tvalid_1's l2: 0.0102606\n",
      "[2000]\ttraining's rmse: 0.084352\ttraining's l2: 0.00711526\tvalid_1's rmse: 0.101037\tvalid_1's l2: 0.0102084\n",
      "[2200]\ttraining's rmse: 0.0828163\ttraining's l2: 0.00685854\tvalid_1's rmse: 0.100861\tvalid_1's l2: 0.010173\n",
      "[2400]\ttraining's rmse: 0.0813689\ttraining's l2: 0.0066209\tvalid_1's rmse: 0.100751\tvalid_1's l2: 0.0101508\n",
      "[2600]\ttraining's rmse: 0.0799664\ttraining's l2: 0.00639462\tvalid_1's rmse: 0.100602\tvalid_1's l2: 0.0101208\n",
      "[2800]\ttraining's rmse: 0.0787037\ttraining's l2: 0.00619427\tvalid_1's rmse: 0.10046\tvalid_1's l2: 0.0100921\n",
      "[3000]\ttraining's rmse: 0.0775563\ttraining's l2: 0.00601497\tvalid_1's rmse: 0.100283\tvalid_1's l2: 0.0100566\n",
      "[3200]\ttraining's rmse: 0.0764226\ttraining's l2: 0.00584041\tvalid_1's rmse: 0.100163\tvalid_1's l2: 0.0100327\n",
      "[3400]\ttraining's rmse: 0.0753065\ttraining's l2: 0.00567107\tvalid_1's rmse: 0.100119\tvalid_1's l2: 0.0100239\n",
      "[3600]\ttraining's rmse: 0.0742135\ttraining's l2: 0.00550764\tvalid_1's rmse: 0.0999895\tvalid_1's l2: 0.00999789\n",
      "[3800]\ttraining's rmse: 0.0731561\ttraining's l2: 0.00535182\tvalid_1's rmse: 0.0999331\tvalid_1's l2: 0.00998663\n",
      "[4000]\ttraining's rmse: 0.0721642\ttraining's l2: 0.00520768\tvalid_1's rmse: 0.0998082\tvalid_1's l2: 0.00996169\n",
      "[4200]\ttraining's rmse: 0.0712732\ttraining's l2: 0.00507987\tvalid_1's rmse: 0.0997801\tvalid_1's l2: 0.00995608\n",
      "score_list = [1306.1124924973867]\n",
      "35068\n",
      "8.729723218012834 4.510461075505404\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.01, min_child_weight=1 will be ignored. Current value: min_sum_hessian_in_leaf=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hyj\\.conda\\envs\\python38\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\hyj\\.conda\\envs\\python38\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttraining's rmse: 0.126115\ttraining's l2: 0.015905\tvalid_1's rmse: 0.112903\tvalid_1's l2: 0.0127471\n",
      "[400]\ttraining's rmse: 0.1057\ttraining's l2: 0.0111725\tvalid_1's rmse: 0.10389\tvalid_1's l2: 0.0107931\n",
      "[600]\ttraining's rmse: 0.0987979\ttraining's l2: 0.00976103\tvalid_1's rmse: 0.101074\tvalid_1's l2: 0.010216\n",
      "[800]\ttraining's rmse: 0.0948753\ttraining's l2: 0.00900132\tvalid_1's rmse: 0.0990868\tvalid_1's l2: 0.00981819\n",
      "[1000]\ttraining's rmse: 0.0918181\ttraining's l2: 0.00843056\tvalid_1's rmse: 0.0981058\tvalid_1's l2: 0.00962475\n",
      "[1200]\ttraining's rmse: 0.0894362\ttraining's l2: 0.00799883\tvalid_1's rmse: 0.0977827\tvalid_1's l2: 0.00956146\n",
      "[1400]\ttraining's rmse: 0.0871566\ttraining's l2: 0.00759627\tvalid_1's rmse: 0.0975145\tvalid_1's l2: 0.00950909\n",
      "[1600]\ttraining's rmse: 0.0851723\ttraining's l2: 0.00725432\tvalid_1's rmse: 0.0973445\tvalid_1's l2: 0.00947595\n",
      "score_list = [1306.1124924973867, 1102.426203156649]\n",
      "0.8296900620619794\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "score_list = []\n",
    "for label in labels:\n",
    "    train[label] = train[label].fillna(0)\n",
    "    tmp = train[~train[label].isna()].reset_index(drop=True).copy(deep=True)\n",
    "    print(len(tmp))\n",
    "    print(np.max(tmp[label]), np.min(tmp[label]))\n",
    "    test_size = 4000\n",
    "    X_train = tmp[features][:-test_size]\n",
    "    X_test = tmp[features][-test_size:]\n",
    "    y_train = tmp[label][:-test_size]\n",
    "    y_test = tmp[label][-test_size:]\n",
    "    if label == \"Label1\":\n",
    "        model = lgb.LGBMRegressor(\n",
    "            boosting=\"gbdt\",\n",
    "            max_depth=4,\n",
    "            learning_rate=0.1,\n",
    "            n_estimators=10000,\n",
    "            min_data_in_leaf=50,\n",
    "            subsample = 0.8,\n",
    "            feature_fraction=0.4,\n",
    "            bagging_seed=1,\n",
    "            reg_alpha=1,\n",
    "            reg_lambda=1,  # 此处不改了\n",
    "            min_sum_hessian_in_leaf=0.01,\n",
    "            random_state=1212\n",
    "        )\n",
    "        model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], eval_metric=['rmse'],\n",
    "              early_stopping_rounds=150, verbose=200)\n",
    "        test_pred = model.predict(X_test)\n",
    "        score_list.append(np.sqrt(mean_squared_error(inverse_transform(y_test), inverse_transform(test_pred))))\n",
    "        print(f\"score_list = {score_list}\")\n",
    "\n",
    "    else:\n",
    "        model = lgb.LGBMRegressor(\n",
    "            boosting=\"gbdt\",\n",
    "            max_depth=4,\n",
    "            learning_rate=0.01,\n",
    "            n_estimators=10000,\n",
    "            min_child_weight=1,\n",
    "            min_data_in_leaf=40,\n",
    "            subsample = 0.4,\n",
    "            feature_fraction=0.4,\n",
    "            bagging_seed=11,\n",
    "            reg_alpha=1,\n",
    "            reg_lambda=0.1,  # 此处不改了\n",
    "            min_sum_hessian_in_leaf=0.01,\n",
    "            random_state=222\n",
    "        )\n",
    "\n",
    "        model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], eval_metric=['rmse'],\n",
    "              early_stopping_rounds=150, verbose=200)\n",
    "        test_pred = model.predict(X_test)\n",
    "        score_list.append(np.sqrt(mean_squared_error(inverse_transform(y_test, c=label_c), inverse_transform(test_pred, c=label_c))))\n",
    "        print(f\"score_list = {score_list}\")\n",
    "\n",
    "loss = np.mean(score_list)\n",
    "score = 1000/(1+loss)\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8238015083098352\n"
     ]
    }
   ],
   "source": [
    "loss = np.mean(score_list)\n",
    "score = 1000/(1+loss)\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}