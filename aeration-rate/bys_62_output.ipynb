{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/train_dataset.csv\")\n",
    "\n",
    "data['is_train'] = True\n",
    "evaluation = pd.read_csv(\"./data/evaluation_public.csv\")\n",
    "evaluation['is_train'] = False\n",
    "sample = pd.read_csv(\"./data/sample_submission.csv\")\n",
    "\n",
    "data = pd.concat([data, evaluation]).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 特征处理"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "data['date'] = pd.to_datetime(data['time'])\n",
    "data['hour'] = data['date'].dt.hour\n",
    "data['year'] = data['date'].dt.year\n",
    "data['month'] = data['date'].dt.month\n",
    "data['minute'] = data['date'].dt.minute\n",
    "data['weekday'] = data['date'].dt.weekday\n",
    "data['day'] = data['date'].dt.day\n",
    "data['hour'] = data['date'].dt.hour\n",
    "# data['ts'] = data['hour']*3600 + data['minute']*60 + data['date'].dt.second\n",
    "data['ts'] = data['hour']*60 + data['minute']\n",
    "# data['ts_7'] = data['ts'].apply(lambda x: x%7)\n",
    "# data['ts_11'] = data['ts'].apply(lambda x: x%11)\n",
    "# data['ts_13'] = data['ts'].apply(lambda x: x%13)\n",
    "# data['ts_17'] = data['ts'].apply(lambda x: x%17)\n",
    "# data['ts_23'] = data['ts'].apply(lambda x: x%23)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import gc\n",
    "features = [ 'JS_NH3', 'CS_NH3', 'JS_TN', 'CS_TN', 'JS_LL', 'CS_LL', 'MCCS_NH4', 'MCCS_NO3', 'JS_COD', 'CS_COD', 'JS_SW', 'CS_SW', 'B_HYC_NH4', 'B_HYC_XD', 'B_HYC_MLSS', 'B_HYC_JS_DO', 'B_HYC_DO', 'B_CS_MQ_SSLL', 'B_QY_ORP', 'N_HYC_NH4', 'N_HYC_XD', 'N_HYC_MLSS', 'N_HYC_JS_DO', 'N_HYC_DO', 'N_CS_MQ_SSLL', 'N_QY_ORP','weekday','hour', 'ts']\n",
    "features = [f for f in features if f not in ['time', 'Label1', 'Label2','CS_LL','CS_NH3', 'JS_SW']] #\n",
    "\n",
    "for f in features:\n",
    "    data[f] = data[f].fillna(method='ffill')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 提取特征的diff值"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_44116/3236213592.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_min_name}_cha'] = data[f] - data[f_min_name]\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_44116/3236213592.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_mean_name] = train_rolling.mean().fillna(0).values\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_44116/3236213592.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_max_name] = train_rolling.max().fillna(0).values\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_44116/3236213592.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_min_name] = train_rolling.min().fillna(0).values\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_44116/3236213592.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_std_name] = train_rolling.std().fillna(0).values\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_44116/3236213592.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_corr_name] = train_rolling.corr().fillna(0).values\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_44116/3236213592.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f_skew_name] = train_rolling.skew().fillna(0).values\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_44116/3236213592.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_mean_name}_cha'] = data[f] - data[f_mean_name]\n",
      "C:\\Users\\38125\\AppData\\Local\\Temp/ipykernel_44116/3236213592.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{f}_{f_max_name}_cha'] = data[f] - data[f_max_name]\n"
     ]
    }
   ],
   "source": [
    "df_list = []\n",
    "i = 0\n",
    "add_featuers = []\n",
    "length = 0\n",
    "for f in ['JS_NH3', 'CS_TN', 'JS_LL', 'JS_COD', 'CS_COD', 'B_HYC_NH4', 'B_HYC_XD', 'B_HYC_MLSS', 'B_HYC_JS_DO', 'B_HYC_DO', 'B_CS_MQ_SSLL',\n",
    "          'N_HYC_NH4', 'N_HYC_XD', 'N_HYC_MLSS', 'N_HYC_DO', 'N_CS_MQ_SSLL', 'N_QY_ORP']:\n",
    "    for r in [15]:\n",
    "        train_rolling = data[f].rolling(window=r, center=False)\n",
    "        f_mean_name = 'rolling{}_{}_mean'.format(r,f)\n",
    "        f_max_name = 'rolling{}_{}_max'.format(r,f)\n",
    "        f_min_name = 'rolling{}_{}_min'.format(r,f)\n",
    "        f_std_name = 'rolling{}_{}_std'.format(r,f)\n",
    "        f_corr_name = 'rolling{}_{}_corr'.format(r,f)\n",
    "        f_cov_name = 'rolling{}_{}_cov'.format(r,f)\n",
    "        f_skew_name = 'rolling{}_{}_skew'.format(r,f)\n",
    "        f_kurt_name = 'rolling{}_{}_kurt'.format(r,f)\n",
    "        data[f_mean_name] = train_rolling.mean().fillna(0).values\n",
    "        data[f_max_name] = train_rolling.max().fillna(0).values\n",
    "        data[f_min_name] = train_rolling.min().fillna(0).values\n",
    "        data[f_std_name] = train_rolling.std().fillna(0).values\n",
    "        data[f_corr_name] = train_rolling.corr().fillna(0).values\n",
    "        data[f_skew_name] = train_rolling.skew().fillna(0).values\n",
    "\n",
    "        data[f'{f}_{f_mean_name}_cha'] = data[f] - data[f_mean_name]\n",
    "        data[f'{f}_{f_max_name}_cha'] = data[f] - data[f_max_name]\n",
    "        data[f'{f}_{f_min_name}_cha'] = data[f] - data[f_min_name]\n",
    "\n",
    "        if i == 0:\n",
    "            add_featuers.append(f_mean_name)\n",
    "            add_featuers.append(f_max_name)\n",
    "            add_featuers.append(f_min_name)\n",
    "            add_featuers.append(f_std_name)\n",
    "            add_featuers.append(f_corr_name)\n",
    "            add_featuers.append(f_skew_name)\n",
    "            add_featuers.append(f'{f}_{f_mean_name}_cha')\n",
    "            add_featuers.append(f'{f}_{f_max_name}_cha')\n",
    "            add_featuers.append(f'{f}_{f_min_name}_cha')\n",
    "features.extend(add_featuers)\n",
    "\n",
    "# 对所有的特征进行划分\n",
    "for f in features:\n",
    "    if f not in ['weekday','hour', 'ts']:\n",
    "        q = len(data[f].drop_duplicates())\n",
    "\n",
    "        data[f] = pd.qcut(data[f], q=int(q/10), labels=False, duplicates=\"drop\")\n",
    "\n",
    "all_train = data[data['is_train']].reset_index(drop=True)\n",
    "test = data[~data['is_train']].reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 删除分布不均衡的特征"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "all_train = all_train.dropna(subset=['Label1', 'Label2']).reset_index(drop=True)\n",
    "def transform(x: pd.Series, c=20):\n",
    "    return np.log1p(x/c)\n",
    "\n",
    "def inverse_transform(x: pd.Series, c = 20):\n",
    "    return np.expm1(x)*c\n",
    "\n",
    "all_train = all_train.dropna(subset=['Label1', 'Label2']).reset_index(drop=True)\n",
    "all_train['Label1'] = transform(all_train['Label1'])\n",
    "label_c = 8\n",
    "all_train['Label2'] = transform(all_train['Label2'], c=label_c)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | featur... | learni... | max_depth | min_da... | min_su... | reg_alpha | reg_la... |\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5789370001020426, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5789370001020426\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=97, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=97\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.0002953253044028762, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.0002953253044028762\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\ttraining's rmse: 0.0852217\ttraining's l2: 0.00726275\tvalid_1's rmse: 0.103938\tvalid_1's l2: 0.0108032\n",
      "[400]\ttraining's rmse: 0.0735274\ttraining's l2: 0.00540628\tvalid_1's rmse: 0.101572\tvalid_1's l2: 0.0103168\n",
      "[600]\ttraining's rmse: 0.0660506\ttraining's l2: 0.00436268\tvalid_1's rmse: 0.100636\tvalid_1's l2: 0.0101276\n",
      "[800]\ttraining's rmse: 0.0602738\ttraining's l2: 0.00363293\tvalid_1's rmse: 0.100356\tvalid_1's l2: 0.0100712\n",
      "[1000]\ttraining's rmse: 0.0554658\ttraining's l2: 0.00307646\tvalid_1's rmse: 0.100276\tvalid_1's l2: 0.0100553\n",
      "Early stopping, best iteration is:\n",
      "[857]\ttraining's rmse: 0.058781\ttraining's l2: 0.0034552\tvalid_1's rmse: 0.100261\tvalid_1's l2: 0.0100522\n",
      "score_list = 1313.2652888442892\n",
      "| \u001B[0m 1       \u001B[0m | \u001B[0m-1.313e+0\u001B[0m | \u001B[0m 0.5789  \u001B[0m | \u001B[0m 0.03464 \u001B[0m | \u001B[0m 8.616   \u001B[0m | \u001B[0m 97.31   \u001B[0m | \u001B[0m 0.000295\u001B[0m | \u001B[0m 0.4475  \u001B[0m | \u001B[0m 0.06743 \u001B[0m |\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.46952796059079793, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.46952796059079793\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=123, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=123\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.0002377264539000228, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.0002377264539000228\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\ttraining's rmse: 0.0815805\ttraining's l2: 0.00665539\tvalid_1's rmse: 0.10233\tvalid_1's l2: 0.0104714\n",
      "[400]\ttraining's rmse: 0.0697965\ttraining's l2: 0.00487156\tvalid_1's rmse: 0.100639\tvalid_1's l2: 0.0101282\n",
      "[600]\ttraining's rmse: 0.0621539\ttraining's l2: 0.00386311\tvalid_1's rmse: 0.100623\tvalid_1's l2: 0.0101251\n",
      "Early stopping, best iteration is:\n",
      "[570]\ttraining's rmse: 0.0631546\ttraining's l2: 0.00398851\tvalid_1's rmse: 0.100517\tvalid_1's l2: 0.0101037\n",
      "score_list = 1312.7215182089974\n",
      "| \u001B[0m 2       \u001B[0m | \u001B[0m-1.313e+0\u001B[0m | \u001B[0m 0.4695  \u001B[0m | \u001B[0m 0.0485  \u001B[0m | \u001B[0m 8.026   \u001B[0m | \u001B[0m 123.9   \u001B[0m | \u001B[0m 0.000237\u001B[0m | \u001B[0m 0.2436  \u001B[0m | \u001B[0m 2.009   \u001B[0m |\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5841084640583973, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5841084640583973\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=19, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=19\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=7.395756414320421e-05, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=7.395756414320421e-05\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\ttraining's rmse: 0.0748821\ttraining's l2: 0.00560734\tvalid_1's rmse: 0.101847\tvalid_1's l2: 0.0103728\n",
      "[400]\ttraining's rmse: 0.0605994\ttraining's l2: 0.00367229\tvalid_1's rmse: 0.100236\tvalid_1's l2: 0.0100472\n",
      "Early stopping, best iteration is:\n",
      "[396]\ttraining's rmse: 0.0608364\ttraining's l2: 0.00370107\tvalid_1's rmse: 0.100214\tvalid_1's l2: 0.0100428\n",
      "score_list = 1309.8936672233942\n",
      "| \u001B[0m 3       \u001B[0m | \u001B[0m-1.313e+0\u001B[0m | \u001B[0m 0.5841  \u001B[0m | \u001B[0m 0.05558 \u001B[0m | \u001B[0m 8.797   \u001B[0m | \u001B[0m 19.2    \u001B[0m | \u001B[0m 7.396e-0\u001B[0m | \u001B[0m 2.702   \u001B[0m | \u001B[0m 2.382   \u001B[0m |\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7091246997728403, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7091246997728403\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=74\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.0008137669170364877, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.0008137669170364877\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\ttraining's rmse: 0.0559048\ttraining's l2: 0.00312535\tvalid_1's rmse: 0.106657\tvalid_1's l2: 0.0113757\n",
      "Early stopping, best iteration is:\n",
      "[209]\ttraining's rmse: 0.0550079\ttraining's l2: 0.00302587\tvalid_1's rmse: 0.106575\tvalid_1's l2: 0.0113583\n",
      "score_list = 1309.8936672233942\n",
      "| \u001B[0m 4       \u001B[0m | \u001B[0m-1.313e+0\u001B[0m | \u001B[0m 0.7091  \u001B[0m | \u001B[0m 0.1632  \u001B[0m | \u001B[0m 11.95   \u001B[0m | \u001B[0m 74.27   \u001B[0m | \u001B[0m 0.000813\u001B[0m | \u001B[0m 1.264   \u001B[0m | \u001B[0m 0.08234 \u001B[0m |\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4888578866003279, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4888578866003279\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=88, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=88\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.0005652854013397265, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.0005652854013397265\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\ttraining's rmse: 0.0932978\ttraining's l2: 0.00870447\tvalid_1's rmse: 0.106112\tvalid_1's l2: 0.0112598\n",
      "[400]\ttraining's rmse: 0.0812692\ttraining's l2: 0.00660468\tvalid_1's rmse: 0.102974\tvalid_1's l2: 0.0106036\n",
      "[600]\ttraining's rmse: 0.0739485\ttraining's l2: 0.00546838\tvalid_1's rmse: 0.101719\tvalid_1's l2: 0.0103467\n",
      "[800]\ttraining's rmse: 0.0684871\ttraining's l2: 0.00469048\tvalid_1's rmse: 0.101379\tvalid_1's l2: 0.0102777\n",
      "[1000]\ttraining's rmse: 0.0641883\ttraining's l2: 0.00412014\tvalid_1's rmse: 0.101068\tvalid_1's l2: 0.0102148\n",
      "[1200]\ttraining's rmse: 0.0603551\ttraining's l2: 0.00364274\tvalid_1's rmse: 0.10089\tvalid_1's l2: 0.0101788\n",
      "[1400]\ttraining's rmse: 0.057106\ttraining's l2: 0.0032611\tvalid_1's rmse: 0.100891\tvalid_1's l2: 0.010179\n",
      "Early stopping, best iteration is:\n",
      "[1318]\ttraining's rmse: 0.058358\ttraining's l2: 0.00340566\tvalid_1's rmse: 0.100855\tvalid_1's l2: 0.0101718\n",
      "score_list = 1309.8936672233942\n",
      "| \u001B[0m 5       \u001B[0m | \u001B[0m-1.313e+0\u001B[0m | \u001B[0m 0.4889  \u001B[0m | \u001B[0m 0.02196 \u001B[0m | \u001B[0m 10.9    \u001B[0m | \u001B[0m 88.73   \u001B[0m | \u001B[0m 0.000565\u001B[0m | \u001B[0m 0.8227  \u001B[0m | \u001B[0m 2.995   \u001B[0m |\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6370509899071032, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6370509899071032\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.0008763843670657037, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.0008763843670657037\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\ttraining's rmse: 0.0502424\ttraining's l2: 0.0025243\tvalid_1's rmse: 0.108514\tvalid_1's l2: 0.0117752\n",
      "Early stopping, best iteration is:\n",
      "[160]\ttraining's rmse: 0.0539505\ttraining's l2: 0.00291065\tvalid_1's rmse: 0.10835\tvalid_1's l2: 0.0117397\n",
      "score_list = 1309.8936672233942\n",
      "| \u001B[0m 6       \u001B[0m | \u001B[0m-1.313e+0\u001B[0m | \u001B[0m 0.6371  \u001B[0m | \u001B[0m 0.1987  \u001B[0m | \u001B[0m 6.897   \u001B[0m | \u001B[0m 5.044   \u001B[0m | \u001B[0m 0.000876\u001B[0m | \u001B[0m 1.916   \u001B[0m | \u001B[0m 0.2146  \u001B[0m |\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.646105943651582, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.646105943651582\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.00032174820630304947, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.00032174820630304947\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\ttraining's rmse: 0.10854\ttraining's l2: 0.0117809\tvalid_1's rmse: 0.108392\tvalid_1's l2: 0.0117489\n",
      "[400]\ttraining's rmse: 0.0846105\ttraining's l2: 0.00715894\tvalid_1's rmse: 0.105944\tvalid_1's l2: 0.0112241\n",
      "[600]\ttraining's rmse: 0.0744942\ttraining's l2: 0.00554939\tvalid_1's rmse: 0.103974\tvalid_1's l2: 0.0108105\n",
      "[800]\ttraining's rmse: 0.0678142\ttraining's l2: 0.00459876\tvalid_1's rmse: 0.102702\tvalid_1's l2: 0.0105477\n",
      "[1000]\ttraining's rmse: 0.0629119\ttraining's l2: 0.0039579\tvalid_1's rmse: 0.101962\tvalid_1's l2: 0.0103962\n",
      "[1200]\ttraining's rmse: 0.0591623\ttraining's l2: 0.00350018\tvalid_1's rmse: 0.101403\tvalid_1's l2: 0.0102826\n",
      "[1400]\ttraining's rmse: 0.0563198\ttraining's l2: 0.00317192\tvalid_1's rmse: 0.101101\tvalid_1's l2: 0.0102214\n",
      "[1600]\ttraining's rmse: 0.0539656\ttraining's l2: 0.00291229\tvalid_1's rmse: 0.100938\tvalid_1's l2: 0.0101886\n",
      "[1800]\ttraining's rmse: 0.0519966\ttraining's l2: 0.00270364\tvalid_1's rmse: 0.100822\tvalid_1's l2: 0.0101652\n",
      "[2000]\ttraining's rmse: 0.050334\ttraining's l2: 0.00253351\tvalid_1's rmse: 0.100859\tvalid_1's l2: 0.0101724\n",
      "Early stopping, best iteration is:\n",
      "[1907]\ttraining's rmse: 0.0510997\ttraining's l2: 0.00261118\tvalid_1's rmse: 0.100763\tvalid_1's l2: 0.0101532\n",
      "score_list = 1309.8936672233942\n",
      "| \u001B[0m 7       \u001B[0m | \u001B[0m-1.313e+0\u001B[0m | \u001B[0m 0.6461  \u001B[0m | \u001B[0m 0.01084 \u001B[0m | \u001B[0m 11.91   \u001B[0m | \u001B[0m 5.036   \u001B[0m | \u001B[0m 0.000321\u001B[0m | \u001B[0m 0.8394  \u001B[0m | \u001B[0m 1.207   \u001B[0m |\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.557064466706585, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.557064466706585\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=124, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=124\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.0006775726443188878, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.0006775726443188878\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\ttraining's rmse: 0.0754858\ttraining's l2: 0.0056981\tvalid_1's rmse: 0.102842\tvalid_1's l2: 0.0105764\n",
      "Early stopping, best iteration is:\n",
      "[247]\ttraining's rmse: 0.0721635\ttraining's l2: 0.00520758\tvalid_1's rmse: 0.102302\tvalid_1's l2: 0.0104657\n",
      "score_list = 1309.8936672233942\n",
      "| \u001B[0m 8       \u001B[0m | \u001B[0m-1.313e+0\u001B[0m | \u001B[0m 0.5571  \u001B[0m | \u001B[0m 0.09486 \u001B[0m | \u001B[0m 6.714   \u001B[0m | \u001B[0m 124.9   \u001B[0m | \u001B[0m 0.000677\u001B[0m | \u001B[0m 1.753   \u001B[0m | \u001B[0m 2.36    \u001B[0m |\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5153051387636536, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5153051387636536\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.00035346908716301885, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.00035346908716301885\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\ttraining's rmse: 0.0632573\ttraining's l2: 0.00400148\tvalid_1's rmse: 0.101089\tvalid_1's l2: 0.0102189\n",
      "[400]\ttraining's rmse: 0.0511153\ttraining's l2: 0.00261277\tvalid_1's rmse: 0.100125\tvalid_1's l2: 0.010025\n",
      "[600]\ttraining's rmse: 0.0448668\ttraining's l2: 0.00201303\tvalid_1's rmse: 0.0998888\tvalid_1's l2: 0.00997777\n",
      "Early stopping, best iteration is:\n",
      "[605]\ttraining's rmse: 0.0447352\ttraining's l2: 0.00200124\tvalid_1's rmse: 0.0998748\tvalid_1's l2: 0.00997498\n",
      "score_list = 1304.438987379871\n",
      "| \u001B[0m 9       \u001B[0m | \u001B[0m-1.313e+0\u001B[0m | \u001B[0m 0.5153  \u001B[0m | \u001B[0m 0.08312 \u001B[0m | \u001B[0m 6.141   \u001B[0m | \u001B[0m 5.166   \u001B[0m | \u001B[0m 0.000353\u001B[0m | \u001B[0m 1.709   \u001B[0m | \u001B[0m 0.6857  \u001B[0m |\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5696841030820068, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5696841030820068\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=124, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=124\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.00027454946702365593, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.00027454946702365593\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\ttraining's rmse: 0.0776377\ttraining's l2: 0.00602762\tvalid_1's rmse: 0.101427\tvalid_1's l2: 0.0102873\n",
      "[400]\ttraining's rmse: 0.0653465\ttraining's l2: 0.00427016\tvalid_1's rmse: 0.100283\tvalid_1's l2: 0.0100567\n",
      "[600]\ttraining's rmse: 0.0571033\ttraining's l2: 0.00326079\tvalid_1's rmse: 0.100328\tvalid_1's l2: 0.0100657\n",
      "Early stopping, best iteration is:\n",
      "[474]\ttraining's rmse: 0.0620054\ttraining's l2: 0.00384467\tvalid_1's rmse: 0.100136\tvalid_1's l2: 0.0100272\n",
      "score_list = 1304.438987379871\n",
      "| \u001B[0m 10      \u001B[0m | \u001B[0m-1.313e+0\u001B[0m | \u001B[0m 0.5697  \u001B[0m | \u001B[0m 0.06376 \u001B[0m | \u001B[0m 11.88   \u001B[0m | \u001B[0m 125.0   \u001B[0m | \u001B[0m 0.000274\u001B[0m | \u001B[0m 1.909   \u001B[0m | \u001B[0m 2.623   \u001B[0m |\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6090491821964747, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6090491821964747\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=76, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=76\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.00048519375484674634, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.00048519375484674634\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\ttraining's rmse: 0.0523649\ttraining's l2: 0.00274209\tvalid_1's rmse: 0.106509\tvalid_1's l2: 0.0113441\n",
      "Early stopping, best iteration is:\n",
      "[116]\ttraining's rmse: 0.0637103\ttraining's l2: 0.004059\tvalid_1's rmse: 0.106269\tvalid_1's l2: 0.0112931\n",
      "score_list = 1304.438987379871\n",
      "| \u001B[0m 11      \u001B[0m | \u001B[0m-1.313e+0\u001B[0m | \u001B[0m 0.609   \u001B[0m | \u001B[0m 0.1987  \u001B[0m | \u001B[0m 8.122   \u001B[0m | \u001B[0m 76.98   \u001B[0m | \u001B[0m 0.000485\u001B[0m | \u001B[0m 0.331   \u001B[0m | \u001B[0m 2.053   \u001B[0m |\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3682744960572834, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3682744960572834\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=12\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.0009338393497285018, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.0009338393497285018\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\ttraining's rmse: 0.0638129\ttraining's l2: 0.00407209\tvalid_1's rmse: 0.101699\tvalid_1's l2: 0.0103426\n",
      "[400]\ttraining's rmse: 0.0505364\ttraining's l2: 0.00255393\tvalid_1's rmse: 0.100778\tvalid_1's l2: 0.0101562\n",
      "Early stopping, best iteration is:\n",
      "[322]\ttraining's rmse: 0.0544752\ttraining's l2: 0.00296755\tvalid_1's rmse: 0.100662\tvalid_1's l2: 0.0101329\n",
      "score_list = 1304.438987379871\n",
      "| \u001B[0m 12      \u001B[0m | \u001B[0m-1.313e+0\u001B[0m | \u001B[0m 0.3683  \u001B[0m | \u001B[0m 0.06777 \u001B[0m | \u001B[0m 8.72    \u001B[0m | \u001B[0m 12.78   \u001B[0m | \u001B[0m 0.000933\u001B[0m | \u001B[0m 0.6522  \u001B[0m | \u001B[0m 0.03972 \u001B[0m |\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4244032146820301, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4244032146820301\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=54, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=54\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.0006276966889682254, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.0006276966889682254\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\ttraining's rmse: 0.0588684\ttraining's l2: 0.00346549\tvalid_1's rmse: 0.106463\tvalid_1's l2: 0.0113343\n",
      "Early stopping, best iteration is:\n",
      "[220]\ttraining's rmse: 0.0571433\ttraining's l2: 0.00326536\tvalid_1's rmse: 0.105841\tvalid_1's l2: 0.0112022\n",
      "score_list = 1304.438987379871\n",
      "| \u001B[0m 13      \u001B[0m | \u001B[0m-1.313e+0\u001B[0m | \u001B[0m 0.4244  \u001B[0m | \u001B[0m 0.1303  \u001B[0m | \u001B[0m 9.812   \u001B[0m | \u001B[0m 54.68   \u001B[0m | \u001B[0m 0.000627\u001B[0m | \u001B[0m 0.5235  \u001B[0m | \u001B[0m 1.55    \u001B[0m |\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.38852038318225035, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.38852038318225035\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=107, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=107\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.0003648799547027185, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.0003648799547027185\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\ttraining's rmse: 0.067412\ttraining's l2: 0.00454438\tvalid_1's rmse: 0.101868\tvalid_1's l2: 0.010377\n",
      "Early stopping, best iteration is:\n",
      "[147]\ttraining's rmse: 0.0728603\ttraining's l2: 0.00530863\tvalid_1's rmse: 0.101636\tvalid_1's l2: 0.0103299\n",
      "score_list = 1304.438987379871\n",
      "| \u001B[0m 14      \u001B[0m | \u001B[0m-1.313e+0\u001B[0m | \u001B[0m 0.3885  \u001B[0m | \u001B[0m 0.141   \u001B[0m | \u001B[0m 7.519   \u001B[0m | \u001B[0m 107.1   \u001B[0m | \u001B[0m 0.000364\u001B[0m | \u001B[0m 2.848   \u001B[0m | \u001B[0m 0.8778  \u001B[0m |\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5643238339767082, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5643238339767082\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=87, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=87\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.0008856765934219574, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.0008856765934219574\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\ttraining's rmse: 0.0941889\ttraining's l2: 0.00887155\tvalid_1's rmse: 0.105885\tvalid_1's l2: 0.0112115\n",
      "[400]\ttraining's rmse: 0.0838446\ttraining's l2: 0.00702992\tvalid_1's rmse: 0.102451\tvalid_1's l2: 0.0104963\n",
      "[600]\ttraining's rmse: 0.0775854\ttraining's l2: 0.00601949\tvalid_1's rmse: 0.101135\tvalid_1's l2: 0.0102284\n",
      "[800]\ttraining's rmse: 0.0726756\ttraining's l2: 0.00528175\tvalid_1's rmse: 0.100843\tvalid_1's l2: 0.0101693\n",
      "[1000]\ttraining's rmse: 0.0685602\ttraining's l2: 0.0047005\tvalid_1's rmse: 0.10043\tvalid_1's l2: 0.0100863\n",
      "[1200]\ttraining's rmse: 0.0651391\ttraining's l2: 0.0042431\tvalid_1's rmse: 0.100319\tvalid_1's l2: 0.0100638\n",
      "[1400]\ttraining's rmse: 0.0619009\ttraining's l2: 0.00383172\tvalid_1's rmse: 0.100285\tvalid_1's l2: 0.0100571\n",
      "Early stopping, best iteration is:\n",
      "[1294]\ttraining's rmse: 0.0635518\ttraining's l2: 0.00403884\tvalid_1's rmse: 0.100211\tvalid_1's l2: 0.0100422\n",
      "score_list = 1304.438987379871\n",
      "| \u001B[0m 15      \u001B[0m | \u001B[0m-1.313e+0\u001B[0m | \u001B[0m 0.5643  \u001B[0m | \u001B[0m 0.02402 \u001B[0m | \u001B[0m 6.458   \u001B[0m | \u001B[0m 87.65   \u001B[0m | \u001B[0m 0.000885\u001B[0m | \u001B[0m 2.398   \u001B[0m | \u001B[0m 2.651   \u001B[0m |\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7161400926423771, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7161400926423771\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=81, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=81\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.0008931100625033861, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.0008931100625033861\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\ttraining's rmse: 0.0832174\ttraining's l2: 0.00692513\tvalid_1's rmse: 0.104108\tvalid_1's l2: 0.0108385\n",
      "[400]\ttraining's rmse: 0.0702961\ttraining's l2: 0.00494155\tvalid_1's rmse: 0.102162\tvalid_1's l2: 0.0104371\n",
      "[600]\ttraining's rmse: 0.062374\ttraining's l2: 0.00389052\tvalid_1's rmse: 0.101562\tvalid_1's l2: 0.0103147\n",
      "[800]\ttraining's rmse: 0.0562356\ttraining's l2: 0.00316244\tvalid_1's rmse: 0.101433\tvalid_1's l2: 0.0102886\n",
      "Early stopping, best iteration is:\n",
      "[708]\ttraining's rmse: 0.0587508\ttraining's l2: 0.00345166\tvalid_1's rmse: 0.10131\tvalid_1's l2: 0.0102637\n",
      "score_list = 1304.438987379871\n",
      "| \u001B[0m 16      \u001B[0m | \u001B[0m-1.313e+0\u001B[0m | \u001B[0m 0.7161  \u001B[0m | \u001B[0m 0.03642 \u001B[0m | \u001B[0m 11.01   \u001B[0m | \u001B[0m 81.18   \u001B[0m | \u001B[0m 0.000893\u001B[0m | \u001B[0m 0.3142  \u001B[0m | \u001B[0m 1.409   \u001B[0m |\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6886988671097447, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6886988671097447\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.0002313764526405403, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.0002313764526405403\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\ttraining's rmse: 0.0537664\ttraining's l2: 0.00289083\tvalid_1's rmse: 0.106788\tvalid_1's l2: 0.0114037\n",
      "Early stopping, best iteration is:\n",
      "[91]\ttraining's rmse: 0.0675657\ttraining's l2: 0.00456512\tvalid_1's rmse: 0.105862\tvalid_1's l2: 0.0112068\n",
      "score_list = 1304.438987379871\n",
      "| \u001B[0m 17      \u001B[0m | \u001B[0m-1.313e+0\u001B[0m | \u001B[0m 0.6887  \u001B[0m | \u001B[0m 0.1677  \u001B[0m | \u001B[0m 6.372   \u001B[0m | \u001B[0m 5.059   \u001B[0m | \u001B[0m 0.000231\u001B[0m | \u001B[0m 2.265   \u001B[0m | \u001B[0m 1.939   \u001B[0m |\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5268013637098177, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5268013637098177\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=124, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=124\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.00020585655327042297, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.00020585655327042297\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\ttraining's rmse: 0.10136\ttraining's l2: 0.0102738\tvalid_1's rmse: 0.106849\tvalid_1's l2: 0.0114167\n",
      "[400]\ttraining's rmse: 0.0877012\ttraining's l2: 0.00769151\tvalid_1's rmse: 0.10383\tvalid_1's l2: 0.0107807\n",
      "[600]\ttraining's rmse: 0.0810998\ttraining's l2: 0.00657718\tvalid_1's rmse: 0.102089\tvalid_1's l2: 0.0104222\n",
      "[800]\ttraining's rmse: 0.0762306\ttraining's l2: 0.0058111\tvalid_1's rmse: 0.101294\tvalid_1's l2: 0.0102605\n",
      "[1000]\ttraining's rmse: 0.0724032\ttraining's l2: 0.00524222\tvalid_1's rmse: 0.10082\tvalid_1's l2: 0.0101647\n",
      "[1200]\ttraining's rmse: 0.069219\ttraining's l2: 0.00479127\tvalid_1's rmse: 0.100595\tvalid_1's l2: 0.0101194\n",
      "[1400]\ttraining's rmse: 0.0663982\ttraining's l2: 0.00440872\tvalid_1's rmse: 0.100331\tvalid_1's l2: 0.0100663\n",
      "[1600]\ttraining's rmse: 0.0638721\ttraining's l2: 0.00407964\tvalid_1's rmse: 0.100204\tvalid_1's l2: 0.0100408\n",
      "[1800]\ttraining's rmse: 0.0615607\ttraining's l2: 0.00378971\tvalid_1's rmse: 0.100101\tvalid_1's l2: 0.0100201\n",
      "[2000]\ttraining's rmse: 0.0594463\ttraining's l2: 0.00353386\tvalid_1's rmse: 0.0999558\tvalid_1's l2: 0.00999115\n",
      "[2200]\ttraining's rmse: 0.0574877\ttraining's l2: 0.00330484\tvalid_1's rmse: 0.0999267\tvalid_1's l2: 0.00998534\n",
      "Early stopping, best iteration is:\n",
      "[2124]\ttraining's rmse: 0.0581991\ttraining's l2: 0.00338714\tvalid_1's rmse: 0.0998827\tvalid_1's l2: 0.00997655\n",
      "score_list = 1304.438987379871\n",
      "| \u001B[0m 18      \u001B[0m | \u001B[0m-1.313e+0\u001B[0m | \u001B[0m 0.5268  \u001B[0m | \u001B[0m 0.01569 \u001B[0m | \u001B[0m 11.83   \u001B[0m | \u001B[0m 125.0   \u001B[0m | \u001B[0m 0.000205\u001B[0m | \u001B[0m 1.005   \u001B[0m | \u001B[0m 0.2511  \u001B[0m |\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.39888339571646697, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.39888339571646697\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.0004215355531118354, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.0004215355531118354\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\ttraining's rmse: 0.0431619\ttraining's l2: 0.00186295\tvalid_1's rmse: 0.107953\tvalid_1's l2: 0.0116539\n",
      "Early stopping, best iteration is:\n",
      "[77]\ttraining's rmse: 0.0610334\ttraining's l2: 0.00372508\tvalid_1's rmse: 0.10608\tvalid_1's l2: 0.011253\n",
      "score_list = 1304.438987379871\n",
      "| \u001B[0m 19      \u001B[0m | \u001B[0m-1.313e+0\u001B[0m | \u001B[0m 0.3989  \u001B[0m | \u001B[0m 0.1865  \u001B[0m | \u001B[0m 10.27   \u001B[0m | \u001B[0m 5.022   \u001B[0m | \u001B[0m 0.000421\u001B[0m | \u001B[0m 0.03919 \u001B[0m | \u001B[0m 2.876   \u001B[0m |\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.46605218750812627, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.46605218750812627\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=124, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=124\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.0005163542457097988, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.0005163542457097988\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\ttraining's rmse: 0.0809972\ttraining's l2: 0.00656055\tvalid_1's rmse: 0.102787\tvalid_1's l2: 0.0105652\n",
      "[400]\ttraining's rmse: 0.0699541\ttraining's l2: 0.00489358\tvalid_1's rmse: 0.101644\tvalid_1's l2: 0.0103315\n",
      "Early stopping, best iteration is:\n",
      "[387]\ttraining's rmse: 0.0705388\ttraining's l2: 0.00497572\tvalid_1's rmse: 0.101615\tvalid_1's l2: 0.0103257\n",
      "score_list = 1304.438987379871\n",
      "| \u001B[0m 20      \u001B[0m | \u001B[0m-1.313e+0\u001B[0m | \u001B[0m 0.4661  \u001B[0m | \u001B[0m 0.06255 \u001B[0m | \u001B[0m 6.144   \u001B[0m | \u001B[0m 124.9   \u001B[0m | \u001B[0m 0.000516\u001B[0m | \u001B[0m 0.577   \u001B[0m | \u001B[0m 0.7106  \u001B[0m |\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3725838528357892, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3725838528357892\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=97, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=97\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.0005568674324604436, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.0005568674324604436\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\ttraining's rmse: 0.0615658\ttraining's l2: 0.00379035\tvalid_1's rmse: 0.104323\tvalid_1's l2: 0.0108833\n",
      "Early stopping, best iteration is:\n",
      "[141]\ttraining's rmse: 0.0686617\ttraining's l2: 0.00471443\tvalid_1's rmse: 0.103989\tvalid_1's l2: 0.0108136\n",
      "score_list = 1304.438987379871\n",
      "| \u001B[0m 21      \u001B[0m | \u001B[0m-1.313e+0\u001B[0m | \u001B[0m 0.3726  \u001B[0m | \u001B[0m 0.1417  \u001B[0m | \u001B[0m 10.83   \u001B[0m | \u001B[0m 97.28   \u001B[0m | \u001B[0m 0.000556\u001B[0m | \u001B[0m 0.447   \u001B[0m | \u001B[0m 1.875   \u001B[0m |\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.3177607402721143, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.3177607402721143\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=33, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=33\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.0009125589563710421, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.0009125589563710421\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\ttraining's rmse: 0.0930405\ttraining's l2: 0.00865654\tvalid_1's rmse: 0.10534\tvalid_1's l2: 0.0110964\n",
      "[400]\ttraining's rmse: 0.0785007\ttraining's l2: 0.00616237\tvalid_1's rmse: 0.103069\tvalid_1's l2: 0.0106231\n",
      "[600]\ttraining's rmse: 0.0700112\ttraining's l2: 0.00490157\tvalid_1's rmse: 0.101577\tvalid_1's l2: 0.0103178\n",
      "[800]\ttraining's rmse: 0.0639352\ttraining's l2: 0.00408771\tvalid_1's rmse: 0.101079\tvalid_1's l2: 0.010217\n",
      "[1000]\ttraining's rmse: 0.0593688\ttraining's l2: 0.00352465\tvalid_1's rmse: 0.101024\tvalid_1's l2: 0.0102058\n",
      "Early stopping, best iteration is:\n",
      "[873]\ttraining's rmse: 0.0621404\ttraining's l2: 0.00386143\tvalid_1's rmse: 0.100956\tvalid_1's l2: 0.0101922\n",
      "score_list = 1304.438987379871\n",
      "| \u001B[0m 22      \u001B[0m | \u001B[0m-1.313e+0\u001B[0m | \u001B[0m 0.3178  \u001B[0m | \u001B[0m 0.02011 \u001B[0m | \u001B[0m 9.036   \u001B[0m | \u001B[0m 33.51   \u001B[0m | \u001B[0m 0.000912\u001B[0m | \u001B[0m 0.177   \u001B[0m | \u001B[0m 0.2169  \u001B[0m |\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.29875093011173637, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.29875093011173637\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.000527533301208655, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.000527533301208655\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\ttraining's rmse: 0.0577082\ttraining's l2: 0.00333024\tvalid_1's rmse: 0.105286\tvalid_1's l2: 0.0110852\n",
      "Early stopping, best iteration is:\n",
      "[102]\ttraining's rmse: 0.0706253\ttraining's l2: 0.00498793\tvalid_1's rmse: 0.104609\tvalid_1's l2: 0.010943\n",
      "score_list = 1304.438987379871\n",
      "| \u001B[0m 23      \u001B[0m | \u001B[0m-1.313e+0\u001B[0m | \u001B[0m 0.2988  \u001B[0m | \u001B[0m 0.1669  \u001B[0m | \u001B[0m 8.428   \u001B[0m | \u001B[0m 20.55   \u001B[0m | \u001B[0m 0.000527\u001B[0m | \u001B[0m 2.749   \u001B[0m | \u001B[0m 2.225   \u001B[0m |\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.2517733395233527, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2517733395233527\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=114, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=114\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=0.0008627826558032733, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=0.0008627826558032733\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\ttraining's rmse: 0.0628651\ttraining's l2: 0.00395202\tvalid_1's rmse: 0.100073\tvalid_1's l2: 0.0100147\n",
      "Early stopping, best iteration is:\n",
      "[143]\ttraining's rmse: 0.0695092\ttraining's l2: 0.00483153\tvalid_1's rmse: 0.099837\tvalid_1's l2: 0.00996742\n",
      "score_list = 1304.2077530317222\n",
      "| \u001B[0m 24      \u001B[0m | \u001B[0m-1.313e+0\u001B[0m | \u001B[0m 0.2518  \u001B[0m | \u001B[0m 0.1617  \u001B[0m | \u001B[0m 9.652   \u001B[0m | \u001B[0m 114.1   \u001B[0m | \u001B[0m 0.000862\u001B[0m | \u001B[0m 1.362   \u001B[0m | \u001B[0m 0.4964  \u001B[0m |\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5907881079360137, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5907881079360137\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=5, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=5\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=9.596985532570601e-07, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=9.596985532570601e-07\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "[200]\ttraining's rmse: 0.174271\ttraining's l2: 0.0303702\tvalid_1's rmse: 0.123171\tvalid_1's l2: 0.015171\n",
      "[400]\ttraining's rmse: 0.124291\ttraining's l2: 0.0154483\tvalid_1's rmse: 0.109701\tvalid_1's l2: 0.0120343\n",
      "[600]\ttraining's rmse: 0.103353\ttraining's l2: 0.0106818\tvalid_1's rmse: 0.107262\tvalid_1's l2: 0.0115052\n",
      "[800]\ttraining's rmse: 0.0928467\ttraining's l2: 0.00862051\tvalid_1's rmse: 0.106332\tvalid_1's l2: 0.0113064\n",
      "[1000]\ttraining's rmse: 0.0864626\ttraining's l2: 0.00747579\tvalid_1's rmse: 0.105705\tvalid_1's l2: 0.0111735\n",
      "[1200]\ttraining's rmse: 0.0818712\ttraining's l2: 0.00670289\tvalid_1's rmse: 0.105078\tvalid_1's l2: 0.0110414\n",
      "[1400]\ttraining's rmse: 0.0780239\ttraining's l2: 0.00608773\tvalid_1's rmse: 0.104253\tvalid_1's l2: 0.0108687\n",
      "[1600]\ttraining's rmse: 0.0747481\ttraining's l2: 0.00558728\tvalid_1's rmse: 0.10355\tvalid_1's l2: 0.0107225\n",
      "[1800]\ttraining's rmse: 0.0719549\ttraining's l2: 0.0051775\tvalid_1's rmse: 0.102941\tvalid_1's l2: 0.0105969\n",
      "[2000]\ttraining's rmse: 0.06952\ttraining's l2: 0.00483303\tvalid_1's rmse: 0.102447\tvalid_1's l2: 0.0104954\n",
      "[2200]\ttraining's rmse: 0.067366\ttraining's l2: 0.00453817\tvalid_1's rmse: 0.102055\tvalid_1's l2: 0.0104152\n",
      "[2400]\ttraining's rmse: 0.0654489\ttraining's l2: 0.00428356\tvalid_1's rmse: 0.101676\tvalid_1's l2: 0.010338\n",
      "[2600]\ttraining's rmse: 0.0637563\ttraining's l2: 0.00406487\tvalid_1's rmse: 0.101404\tvalid_1's l2: 0.0102827\n",
      "[2800]\ttraining's rmse: 0.0622298\ttraining's l2: 0.00387254\tvalid_1's rmse: 0.10117\tvalid_1's l2: 0.0102354\n",
      "[3000]\ttraining's rmse: 0.0608561\ttraining's l2: 0.00370346\tvalid_1's rmse: 0.100996\tvalid_1's l2: 0.0102002\n",
      "[3200]\ttraining's rmse: 0.0595573\ttraining's l2: 0.00354707\tvalid_1's rmse: 0.100802\tvalid_1's l2: 0.0101611\n",
      "[3400]\ttraining's rmse: 0.0583471\ttraining's l2: 0.00340439\tvalid_1's rmse: 0.10064\tvalid_1's l2: 0.0101285\n",
      "[3600]\ttraining's rmse: 0.0572974\ttraining's l2: 0.003283\tvalid_1's rmse: 0.100501\tvalid_1's l2: 0.0101005\n",
      "[3800]\ttraining's rmse: 0.0563495\ttraining's l2: 0.00317527\tvalid_1's rmse: 0.100407\tvalid_1's l2: 0.0100815\n",
      "[4000]\ttraining's rmse: 0.0554518\ttraining's l2: 0.0030749\tvalid_1's rmse: 0.10034\tvalid_1's l2: 0.0100681\n",
      "[4200]\ttraining's rmse: 0.0546236\ttraining's l2: 0.00298374\tvalid_1's rmse: 0.100297\tvalid_1's l2: 0.0100595\n",
      "[4400]\ttraining's rmse: 0.0538696\ttraining's l2: 0.00290193\tvalid_1's rmse: 0.100223\tvalid_1's l2: 0.0100447\n",
      "[4600]\ttraining's rmse: 0.0531544\ttraining's l2: 0.00282539\tvalid_1's rmse: 0.100159\tvalid_1's l2: 0.0100318\n",
      "[4800]\ttraining's rmse: 0.0524802\ttraining's l2: 0.00275417\tvalid_1's rmse: 0.10011\tvalid_1's l2: 0.0100219\n",
      "[5000]\ttraining's rmse: 0.0518234\ttraining's l2: 0.00268566\tvalid_1's rmse: 0.100095\tvalid_1's l2: 0.0100189\n",
      "[5200]\ttraining's rmse: 0.0512159\ttraining's l2: 0.00262307\tvalid_1's rmse: 0.100066\tvalid_1's l2: 0.0100132\n",
      "Early stopping, best iteration is:\n",
      "[5241]\ttraining's rmse: 0.0511007\ttraining's l2: 0.00261128\tvalid_1's rmse: 0.10006\tvalid_1's l2: 0.010012\n",
      "score_list = 1304.2077530317222\n",
      "| \u001B[0m 25      \u001B[0m | \u001B[0m-1.313e+0\u001B[0m | \u001B[0m 0.5908  \u001B[0m | \u001B[0m 0.004077\u001B[0m | \u001B[0m 11.91   \u001B[0m | \u001B[0m 5.066   \u001B[0m | \u001B[0m 9.597e-0\u001B[0m | \u001B[0m 1.072   \u001B[0m | \u001B[0m 0.594   \u001B[0m |\n",
      "=============================================================================================================\n",
      "0.7457779581396045\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "import warnings\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "labels = ['Label1']#, 'Label2']\n",
    "score_list = []\n",
    "test_size = 4000\n",
    "\n",
    "for label in labels:\n",
    "    X_train = all_train[features][:-test_size]\n",
    "    y_train = all_train[label][:-test_size]\n",
    "\n",
    "    X_test = all_train[features][-test_size:]\n",
    "    y_test = all_train[label][-test_size:]\n",
    "    if label == \"Label1\":\n",
    "        def LGB_bayesian(max_depth,\n",
    "                         learning_rate,\n",
    "                         min_data_in_leaf,\n",
    "                         feature_fraction,\n",
    "                         reg_alpha,\n",
    "                         reg_lambda,\n",
    "                         min_sum_hessian_in_leaf):\n",
    "            model = lgb.LGBMRegressor(\n",
    "                boosting=\"gbdt\",\n",
    "                max_depth=int(max_depth),\n",
    "                learning_rate=learning_rate,\n",
    "                n_estimators=10000,\n",
    "                min_data_in_leaf=int(min_data_in_leaf),\n",
    "                feature_fraction=feature_fraction,\n",
    "                bagging_seed=1212,\n",
    "                reg_alpha=reg_alpha,\n",
    "                reg_lambda=reg_lambda,  # 此处不改了\n",
    "                min_sum_hessian_in_leaf=min_sum_hessian_in_leaf,\n",
    "                random_state=1212\n",
    "            )\n",
    "            model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], eval_metric=['rmse'],\n",
    "                  early_stopping_rounds=150, verbose=200)\n",
    "            test_pred = model.predict(X_test)\n",
    "\n",
    "            score_list.append(np.sqrt(mean_squared_error(inverse_transform(y_test), inverse_transform(test_pred))))\n",
    "            print(f\"score_list = {min(score_list)}\")\n",
    "            return -1*score_list[0]\n",
    "\n",
    "        bounds_LGB = {\n",
    "            'max_depth': (6, 12),\n",
    "            'learning_rate': (0.001, 0.2),\n",
    "            'feature_fraction': (0.23, 0.8),\n",
    "            'min_data_in_leaf': (5, 125),\n",
    "            'reg_alpha': (0.0, 3.0),\n",
    "            'reg_lambda': (0.0, 3.0),\n",
    "            'min_sum_hessian_in_leaf': (0.0, 1e-3)\n",
    "        }\n",
    "        LGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=111)\n",
    "        init_points = 5\n",
    "        n_iter = 20\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('ignore')\n",
    "            LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)\n",
    "\n",
    "\n",
    "        # model2 = lgb.LGBMRegressor(\n",
    "        #     boosting=\"gbdt\",\n",
    "        #     max_depth=4,\n",
    "        #     learning_rate=0.035,\n",
    "        #     n_estimators=int(1.5*model.best_iteration_),\n",
    "        #     min_data_in_leaf=5,\n",
    "        #     feature_fraction=0.3,\n",
    "        #     bagging_seed=1212,\n",
    "        #     reg_alpha=1,\n",
    "        #     reg_lambda=1,  # 此处不改了\n",
    "        #     min_sum_hessian_in_leaf=1e-8,\n",
    "        #     random_state=1212\n",
    "        # )\n",
    "        # model2.fit(all_train[features], all_train[label])\n",
    "        # test[label] = inverse_transform(model2.predict(test[features]))\n",
    "    else:\n",
    "        model = lgb.LGBMRegressor(\n",
    "            boosting=\"gbdt\",\n",
    "            max_depth=4,\n",
    "            learning_rate=0.02,\n",
    "            n_estimators=10000,\n",
    "            min_child_weight=1,\n",
    "            min_data_in_leaf=40,\n",
    "            feature_fraction=0.3,\n",
    "            bagging_seed=1212,\n",
    "            reg_alpha=1,\n",
    "            reg_lambda=1,  # 此处不改了\n",
    "            min_sum_hessian_in_leaf=1e-8,\n",
    "            random_state=222\n",
    "        )\n",
    "\n",
    "        model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], eval_metric=['rmse'],\n",
    "              early_stopping_rounds=150, verbose=200)\n",
    "        test_pred = model.predict(X_test)\n",
    "        score_list.append(np.sqrt(mean_squared_error(inverse_transform(y_test, c=label_c), inverse_transform(test_pred, c=label_c))))\n",
    "        print(f\"score_list = {score_list}\")\n",
    "\n",
    "        # model2 = lgb.LGBMRegressor(\n",
    "        #     boosting=\"gbdt\",\n",
    "        #     max_depth=4,\n",
    "        #     learning_rate=0.02,\n",
    "        #     n_estimators=int(1.5*model.best_iteration_),\n",
    "        #     min_data_in_leaf=40,\n",
    "        #     subsample = 0.4,\n",
    "        #     feature_fraction=0.3,\n",
    "        #     bagging_seed=1212,\n",
    "        #     reg_alpha=1,\n",
    "        #     reg_lambda=1,  # 此处不改了\n",
    "        #     min_sum_hessian_in_leaf=1e-8,\n",
    "        #     random_state=222\n",
    "        # )\n",
    "        # model2.fit(all_train[features], all_train[label])\n",
    "        #\n",
    "        # test[label] = inverse_transform(model2.predict(test[features]), c=label_c)\n",
    "loss = np.mean(score_list)\n",
    "score = 1000/(1+loss)\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = np.mean(score_list)\n",
    "score = 1000/(1+loss)\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test[['time'] + labels].to_csv(f\"./res/lightgbm_random_res.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}